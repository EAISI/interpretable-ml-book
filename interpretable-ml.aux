\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/r>>}
\@writefile{toc}{\contentsline {fm}{Summary}{v}{chapter*.1}}
\newlabel{summary}{{}{v}{Summary}{chapter*.1}{}}
\@writefile{toc}{\contentsline {chapter}{Preface by the Author}{vii}{chapter*.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{preface-by-the-author}{{}{vii}{Preface by the Author}{chapter*.2}{}}
\HyPL@Entry{10<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Story Time}{3}{section.1.1}}
\newlabel{storytime}{{1.1}{3}{Story Time}{section.1.1}{}}
\newlabel{lightning-never-strikes-twice}{{1.1}{3}{Lightning Never Strikes Twice}{section*.3}{}}
\newlabel{trust-fall}{{1.1}{6}{Trust Fall}{section*.4}{}}
\newlabel{fermis-paperclips}{{1.1}{8}{Fermi's Paperclips}{section*.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}What Is Machine Learning?}{11}{section.1.2}}
\newlabel{what-is-machine-learning}{{1.2}{11}{What Is Machine Learning?}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Terminology}{13}{section.1.3}}
\newlabel{terminology}{{1.3}{13}{Terminology}{section.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A learner learns a model from labeled training data. The model is used to make predictions.}}{14}{figure.1.1}}
\newlabel{fig:learner-definition}{{1.1}{14}{A learner learns a model from labeled training data. The model is used to make predictions}{figure.1.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Interpretability}{17}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{interpretability}{{2}{17}{Interpretability}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Importance of Interpretability}{17}{section.2.1}}
\newlabel{interpretability-importance}{{2.1}{17}{Importance of Interpretability}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Recommended products that are frequently bought together.}}{20}{figure.2.1}}
\newlabel{fig:amazon-recommendation}{{2.1}{20}{Recommended products that are frequently bought together}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Doge, our vacuum cleaner, got stuck. As an explanation for the accident, Doge told us that it needs to be on an even surface.}}{22}{figure.2.2}}
\newlabel{fig:doge-stuck}{{2.2}{22}{Doge, our vacuum cleaner, got stuck. As an explanation for the accident, Doge told us that it needs to be on an even surface}{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Taxonomy of Interpretability Methods}{25}{section.2.2}}
\newlabel{taxonomy-of-interpretability-methods}{{2.2}{25}{Taxonomy of Interpretability Methods}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Scope of Interpretability}{27}{section.2.3}}
\newlabel{scope-of-interpretability}{{2.3}{27}{Scope of Interpretability}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Algorithm Transparency}{27}{subsection.2.3.1}}
\newlabel{algorithm-transparency}{{2.3.1}{27}{Algorithm Transparency}{subsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Global, Holistic Model Interpretability}{27}{subsection.2.3.2}}
\newlabel{global-holistic-model-interpretability}{{2.3.2}{27}{Global, Holistic Model Interpretability}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Global Model Interpretability on a Modular Level}{28}{subsection.2.3.3}}
\newlabel{global-model-interpretability-on-a-modular-level}{{2.3.3}{28}{Global Model Interpretability on a Modular Level}{subsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Local Interpretability for a Single Prediction}{29}{subsection.2.3.4}}
\newlabel{local-interpretability-for-a-single-prediction}{{2.3.4}{29}{Local Interpretability for a Single Prediction}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Local Interpretability for a Group of Predictions}{29}{subsection.2.3.5}}
\newlabel{local-interpretability-for-a-group-of-predictions}{{2.3.5}{29}{Local Interpretability for a Group of Predictions}{subsection.2.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Evaluation of Interpretability}{30}{section.2.4}}
\newlabel{evaluation-of-interpretability}{{2.4}{30}{Evaluation of Interpretability}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Properties of Explanations}{31}{section.2.5}}
\newlabel{properties}{{2.5}{31}{Properties of Explanations}{section.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Human-friendly Explanations}{35}{section.2.6}}
\newlabel{explanation}{{2.6}{35}{Human-friendly Explanations}{section.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}What Is an Explanation?}{35}{subsection.2.6.1}}
\newlabel{what-is-an-explanation}{{2.6.1}{35}{What Is an Explanation?}{subsection.2.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}What Is a Good Explanation?}{36}{subsection.2.6.2}}
\newlabel{good-explanation}{{2.6.2}{36}{What Is a Good Explanation?}{subsection.2.6.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Datasets}{43}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{data}{{3}{43}{Datasets}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Bike Rentals (Regression)}{43}{section.3.1}}
\newlabel{bike-data}{{3.1}{43}{Bike Rentals (Regression)}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}YouTube Spam Comments (Text Classification)}{44}{section.3.2}}
\newlabel{spam-data}{{3.2}{44}{YouTube Spam Comments (Text Classification)}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Risk Factors for Cervical Cancer (Classification)}{46}{section.3.3}}
\newlabel{cervical}{{3.3}{46}{Risk Factors for Cervical Cancer (Classification)}{section.3.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Interpretable Models}{49}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{simple}{{4}{49}{Interpretable Models}{chapter.4}{}}
\gdef \LT@i {\LT@entry 
    {2}{103.196pt}\LT@entry 
    {1}{41.776pt}\LT@entry 
    {1}{60.916pt}\LT@entry 
    {1}{65.752pt}\LT@entry 
    {2}{51.728pt}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Linear Regression}{51}{section.4.1}}
\newlabel{limo}{{4.1}{51}{Linear Regression}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Interpretation}{54}{subsection.4.1.1}}
\newlabel{interpretation}{{4.1.1}{54}{Interpretation}{subsection.4.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Example}{57}{subsection.4.1.2}}
\newlabel{example}{{4.1.2}{57}{Example}{subsection.4.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Visual Interpretation}{58}{subsection.4.1.3}}
\newlabel{visual-interpretation}{{4.1.3}{58}{Visual Interpretation}{subsection.4.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3.1}Weight Plot}{58}{subsubsection.4.1.3.1}}
\newlabel{weight-plot}{{4.1.3.1}{58}{Weight Plot}{subsubsection.4.1.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Weights are displayed as points and the 95\% confidence intervals as lines.}}{59}{figure.4.1}}
\newlabel{fig:linear-weights-plot}{{4.1}{59}{Weights are displayed as points and the 95\% confidence intervals as lines}{figure.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3.2}Effect Plot}{59}{subsubsection.4.1.3.2}}
\newlabel{effect-plot}{{4.1.3.2}{59}{Effect Plot}{subsubsection.4.1.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The feature effect plot shows the distribution of effects (= feature value times feature weight) across the data per feature.}}{60}{figure.4.2}}
\newlabel{fig:linear-effects}{{4.2}{60}{The feature effect plot shows the distribution of effects (= feature value times feature weight) across the data per feature}{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Explain Individual Predictions}{61}{subsection.4.1.4}}
\newlabel{explain-individual-predictions}{{4.1.4}{61}{Explain Individual Predictions}{subsection.4.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest.}}{62}{figure.4.3}}
\newlabel{fig:linear-effects-single}{{4.3}{62}{The effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Encoding of Categorical Features}{63}{subsection.4.1.5}}
\newlabel{cat-code}{{4.1.5}{63}{Encoding of Categorical Features}{subsection.4.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Do Linear Models Create Good Explanations?}{65}{subsection.4.1.6}}
\newlabel{do-linear-models-create-good-explanations}{{4.1.6}{65}{Do Linear Models Create Good Explanations?}{subsection.4.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.7}Sparse Linear Models}{65}{subsection.4.1.7}}
\newlabel{sparse-linear}{{4.1.7}{65}{Sparse Linear Models}{subsection.4.1.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.1}Lasso}{66}{subsubsection.4.1.7.1}}
\newlabel{lasso}{{4.1.7.1}{66}{Lasso}{subsubsection.4.1.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces With increasing penalty of the weights, fewer and fewer features receive a non-zero weight estimate. These curves are also called regularization paths. The number above the plot is the number of non-zero weights.}}{67}{figure.4.4}}
\newlabel{fig:lasso-path}{{4.4}{67}{With increasing penalty of the weights, fewer and fewer features receive a non-zero weight estimate. These curves are also called regularization paths. The number above the plot is the number of non-zero weights}{figure.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.8}Advantages}{69}{subsection.4.1.8}}
\newlabel{advantages}{{4.1.8}{69}{Advantages}{subsection.4.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.9}Disadvantages}{70}{subsection.4.1.9}}
\newlabel{disadvantages}{{4.1.9}{70}{Disadvantages}{subsection.4.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Logistic Regression}{71}{section.4.2}}
\newlabel{logistic}{{4.2}{71}{Logistic Regression}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}What is Wrong with Linear Regression for Classification?}{71}{subsection.4.2.1}}
\newlabel{what-is-wrong-with-linear-regression-for-classification}{{4.2.1}{71}{What is Wrong with Linear Regression for Classification?}{subsection.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces A linear model classifies tumors as malignant (1) or benign (0) given their size. The lines show the prediction of the linear model. For the data on the left, we can use 0.5 as classification threshold. After introducing a few more malignant tumor cases, the regression line shifts and a threshold of 0.5 no longer separates the classes. Points are slightly jittered to reduce over-plotting. }}{72}{figure.4.5}}
\newlabel{fig:linear-class-threshold}{{4.5}{72}{A linear model classifies tumors as malignant (1) or benign (0) given their size. The lines show the prediction of the linear model. For the data on the left, we can use 0.5 as classification threshold. After introducing a few more malignant tumor cases, the regression line shifts and a threshold of 0.5 no longer separates the classes. Points are slightly jittered to reduce over-plotting}{figure.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Theory}{72}{subsection.4.2.2}}
\newlabel{theory}{{4.2.2}{72}{Theory}{subsection.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The logistic function. It outputs numbers between 0 and 1. At input 0, it outputs 0.5.}}{73}{figure.4.6}}
\newlabel{fig:logistic-function}{{4.6}{73}{The logistic function. It outputs numbers between 0 and 1. At input 0, it outputs 0.5}{figure.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The logistic regression model finds the correct decision boundary between malignant and benign depending on tumor size. The line is the logistic function shifted and squeezed to fit the data.}}{74}{figure.4.7}}
\newlabel{fig:logistic-class-threshold}{{4.7}{74}{The logistic regression model finds the correct decision boundary between malignant and benign depending on tumor size. The line is the logistic function shifted and squeezed to fit the data}{figure.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Interpretation}{74}{subsection.4.2.3}}
\newlabel{interpretation-1}{{4.2.3}{74}{Interpretation}{subsection.4.2.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces The results of fitting a logistic regression model on the cervical cancer dataset. Shown are the features used in the model, their estimated weights and corresponding odds ratios, and the standard errors of the estimated weights.}}{77}{table.4.2}}
\newlabel{tab:logistic-example}{{4.2}{77}{The results of fitting a logistic regression model on the cervical cancer dataset. Shown are the features used in the model, their estimated weights and corresponding odds ratios, and the standard errors of the estimated weights}{table.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Example}{77}{subsection.4.2.4}}
\newlabel{example-1}{{4.2.4}{77}{Example}{subsection.4.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Advantages and Disadvantages}{77}{subsection.4.2.5}}
\newlabel{advantages-and-disadvantages}{{4.2.5}{77}{Advantages and Disadvantages}{subsection.4.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Software}{78}{subsection.4.2.6}}
\newlabel{software}{{4.2.6}{78}{Software}{subsection.4.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}GLM, GAM and more}{79}{section.4.3}}
\newlabel{extend-lm}{{4.3}{79}{GLM, GAM and more}{section.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Three assumptions of the linear model (left side): Gaussian distribution of the outcome given the features, additivity (= no interactions) and linear relationship. Reality usually does not adhere to those assumptions (right side): Outcomes might have non-Gaussian distributions, features might interact and the relationship might be nonlinear.}}{80}{figure.4.8}}
\newlabel{fig:three-lm-problems}{{4.8}{80}{Three assumptions of the linear model (left side): Gaussian distribution of the outcome given the features, additivity (= no interactions) and linear relationship. Reality usually does not adhere to those assumptions (right side): Outcomes might have non-Gaussian distributions, features might interact and the relationship might be nonlinear}{figure.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Non-Gaussian Outcomes - GLMs}{81}{subsection.4.3.1}}
\newlabel{glm}{{4.3.1}{81}{Non-Gaussian Outcomes - GLMs}{subsection.4.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Simulated distribution of number of daily coffees for 200 days.}}{85}{figure.4.9}}
\newlabel{fig:poisson-data}{{4.9}{85}{Simulated distribution of number of daily coffees for 200 days}{figure.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Predicted number of coffees dependent on stress, sleep and work. The linear model predicts negative values.}}{86}{figure.4.10}}
\newlabel{fig:failing-linear-model}{{4.10}{86}{Predicted number of coffees dependent on stress, sleep and work. The linear model predicts negative values}{figure.4.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Predicted number of coffees dependent on stress, sleep and work. The GLM with Poisson assumption and log link is an appropriate model for this dataset.}}{87}{figure.4.11}}
\newlabel{fig:linear-model-positive}{{4.11}{87}{Predicted number of coffees dependent on stress, sleep and work. The GLM with Poisson assumption and log link is an appropriate model for this dataset}{figure.4.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Interactions}{88}{subsection.4.3.2}}
\newlabel{lm-interact}{{4.3.2}{88}{Interactions}{subsection.4.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces The effect (including interaction) of temperature and working day on the predicted number of bikes for a linear model. Effectively, we get two slopes for the temperature, one for each category of the working day feature.}}{92}{figure.4.12}}
\newlabel{fig:interaction-plot}{{4.12}{92}{The effect (including interaction) of temperature and working day on the predicted number of bikes for a linear model. Effectively, we get two slopes for the temperature, one for each category of the working day feature}{figure.4.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Nonlinear Effects - GAMs}{92}{subsection.4.3.3}}
\newlabel{gam}{{4.3.3}{92}{Nonlinear Effects - GAMs}{subsection.4.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Predicting the number of rented bicycles using only the temperature feature. A linear model (top left) does not fit the data well. One solution is to transform the feature with e.g. the logarithm (top right), categorize it (bottom left), which is usually a bad decision, or use Generalized Additive Models that can automatically fit a smooth curve for temperature (bottom right).}}{94}{figure.4.13}}
\newlabel{fig:nonlinear-effects}{{4.13}{94}{Predicting the number of rented bicycles using only the temperature feature. A linear model (top left) does not fit the data well. One solution is to transform the feature with e.g. the logarithm (top right), categorize it (bottom left), which is usually a bad decision, or use Generalized Additive Models that can automatically fit a smooth curve for temperature (bottom right)}{figure.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces To smoothly model the temperature effect, we use 4 spline functions. Each temperature value is mapped to (here) 4 spline values. If an instance has a temperature of 30 °C, the value for the first spline feature is -1, for the second 0.7, for the third -0.8 and for the 4th 1.7.}}{97}{figure.4.14}}
\newlabel{fig:splines}{{4.14}{97}{To smoothly model the temperature effect, we use 4 spline functions. Each temperature value is mapped to (here) 4 spline values. If an instance has a temperature of 30 °C, the value for the first spline feature is -1, for the second 0.7, for the third -0.8 and for the 4th 1.7}{figure.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces GAM feature effect of the temperature for predicting the number of rented bikes (temperature used as the only feature).}}{98}{figure.4.15}}
\newlabel{fig:splines-curve}{{4.15}{98}{GAM feature effect of the temperature for predicting the number of rented bikes (temperature used as the only feature)}{figure.4.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Advantages}{98}{subsection.4.3.4}}
\newlabel{advantages-1}{{4.3.4}{98}{Advantages}{subsection.4.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Disadvantages}{99}{subsection.4.3.5}}
\newlabel{disadvantages-1}{{4.3.5}{99}{Disadvantages}{subsection.4.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Software}{99}{subsection.4.3.6}}
\newlabel{software-1}{{4.3.6}{99}{Software}{subsection.4.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Further Extensions}{100}{subsection.4.3.7}}
\newlabel{more-lm-extension}{{4.3.7}{100}{Further Extensions}{subsection.4.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Decision Tree}{102}{section.4.4}}
\newlabel{tree}{{4.4}{102}{Decision Tree}{section.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Decision tree with artificial data. Instances with a value greater than 3 for feature x1 end up in node 5. All other instances are assigned to node 3 or node 4, depending on whether values of feature x2 exceed 1.}}{103}{figure.4.16}}
\newlabel{fig:tree-artificial}{{4.16}{103}{Decision tree with artificial data. Instances with a value greater than 3 for feature x1 end up in node 5. All other instances are assigned to node 3 or node 4, depending on whether values of feature x2 exceed 1}{figure.4.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Interpretation}{104}{subsection.4.4.1}}
\newlabel{interpretation-2}{{4.4.1}{104}{Interpretation}{subsection.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Example}{105}{subsection.4.4.2}}
\newlabel{example-2}{{4.4.2}{105}{Example}{subsection.4.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Regression tree fitted on the bike rental data. The maximum allowed depth for the tree was set to 2. The trend feature (days since 2011) and the temperature (temp) have been selected for the splits. The boxplots show the distribution of bicycle counts in the terminal node.}}{106}{figure.4.17}}
\newlabel{fig:tree-example}{{4.17}{106}{Regression tree fitted on the bike rental data. The maximum allowed depth for the tree was set to 2. The trend feature (days since 2011) and the temperature (temp) have been selected for the splits. The boxplots show the distribution of bicycle counts in the terminal node}{figure.4.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Advantages}{106}{subsection.4.4.3}}
\newlabel{advantages-2}{{4.4.3}{106}{Advantages}{subsection.4.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Importance of the features measured by how much the node purity is improved on average.}}{107}{figure.4.18}}
\newlabel{fig:tree-importance}{{4.18}{107}{Importance of the features measured by how much the node purity is improved on average}{figure.4.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Disadvantages}{108}{subsection.4.4.4}}
\newlabel{disadvantages-2}{{4.4.4}{108}{Disadvantages}{subsection.4.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.5}Software}{109}{subsection.4.4.5}}
\newlabel{software-2}{{4.4.5}{109}{Software}{subsection.4.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Decision Rules}{110}{section.4.5}}
\newlabel{rules}{{4.5}{110}{Decision Rules}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Learn Rules from a Single Feature (OneR)}{113}{subsection.4.5.1}}
\newlabel{learn-rules-from-a-single-feature-oner}{{4.5.1}{113}{Learn Rules from a Single Feature (OneR)}{subsection.4.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Sequential Covering}{117}{subsection.4.5.2}}
\newlabel{sequential-covering}{{4.5.2}{117}{Sequential Covering}{subsection.4.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces The covering algorithm works by sequentially covering the feature space with single rules and removing the data points that are already covered by those rules. For visualization purposes, the features x1 and x2 are continuous, but most rule learning algorithms require categorical features.}}{118}{figure.4.19}}
\newlabel{fig:covering-algo}{{4.19}{118}{The covering algorithm works by sequentially covering the feature space with single rules and removing the data points that are already covered by those rules. For visualization purposes, the features x1 and x2 are continuous, but most rule learning algorithms require categorical features}{figure.4.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Learning a rule by searching a path through a decision tree. A decision tree is grown to predict the target of interest. We start at the root node, greedily and iteratively follow the path which locally produces the purest subset (e.g. highest accuracy) and add all the split values to the rule condition. We end up with: If `location=good` and `size=big`, then `value=high`.}}{119}{figure.4.20}}
\newlabel{fig:learn-one-rule}{{4.20}{119}{Learning a rule by searching a path through a decision tree. A decision tree is grown to predict the target of interest. We start at the root node, greedily and iteratively follow the path which locally produces the purest subset (e.g. highest accuracy) and add all the split values to the rule condition. We end up with: If `location=good` and `size=big`, then `value=high`}{figure.4.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Bayesian Rule Lists}{121}{subsection.4.5.3}}
\newlabel{bayesian-rule-lists}{{4.5.3}{121}{Bayesian Rule Lists}{subsection.4.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Advantages}{129}{subsection.4.5.4}}
\newlabel{advantages-3}{{4.5.4}{129}{Advantages}{subsection.4.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Disadvantages}{130}{subsection.4.5.5}}
\newlabel{disadvantages-3}{{4.5.5}{130}{Disadvantages}{subsection.4.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.6}Software and Alternatives}{131}{subsection.4.5.6}}
\newlabel{software-and-alternatives}{{4.5.6}{131}{Software and Alternatives}{subsection.4.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}RuleFit}{132}{section.4.6}}
\newlabel{rulefit}{{4.6}{132}{RuleFit}{section.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces 4 rules can be generated from a tree with 3 terminal nodes.}}{133}{figure.4.21}}
\newlabel{fig:rulefit-split}{{4.21}{133}{4 rules can be generated from a tree with 3 terminal nodes}{figure.4.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Interpretation and Example}{133}{subsection.4.6.1}}
\newlabel{interpretation-and-example}{{4.6.1}{133}{Interpretation and Example}{subsection.4.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Feature importance measures for a RuleFit model predicting bike counts. The most important features for the predictions were temperature and time trend.}}{135}{figure.4.22}}
\newlabel{fig:rulefit-importance}{{4.22}{135}{Feature importance measures for a RuleFit model predicting bike counts. The most important features for the predictions were temperature and time trend}{figure.4.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Theory}{135}{subsection.4.6.2}}
\newlabel{theory-1}{{4.6.2}{135}{Theory}{subsection.4.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Advantages}{140}{subsection.4.6.3}}
\newlabel{advantages-4}{{4.6.3}{140}{Advantages}{subsection.4.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.4}Disadvantages}{140}{subsection.4.6.4}}
\newlabel{disadvantages-4}{{4.6.4}{140}{Disadvantages}{subsection.4.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.5}Software and Alternative}{141}{subsection.4.6.5}}
\newlabel{software-and-alternative}{{4.6.5}{141}{Software and Alternative}{subsection.4.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Other Interpretable Models}{142}{section.4.7}}
\newlabel{other-interpretable}{{4.7}{142}{Other Interpretable Models}{section.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Naive Bayes Classifier}{142}{subsection.4.7.1}}
\newlabel{naive-bayes-classifier}{{4.7.1}{142}{Naive Bayes Classifier}{subsection.4.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}K-Nearest Neighbors}{143}{subsection.4.7.2}}
\newlabel{k-nearest-neighbors}{{4.7.2}{143}{K-Nearest Neighbors}{subsection.4.7.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Model-Agnostic Methods}{145}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{agnostic}{{5}{145}{Model-Agnostic Methods}{chapter.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in the form of explanations.}}{147}{figure.5.1}}
\newlabel{fig:bigpicture}{{5.1}{147}{The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in the form of explanations}{figure.5.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Example-Based Explanations}{149}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{example-based}{{6}{149}{Example-Based Explanations}{chapter.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Global Model-Agnostic Methods}{153}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{global-methods}{{7}{153}{Global Model-Agnostic Methods}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Partial Dependence Plot (PDP)}{154}{section.7.1}}
\newlabel{pdp}{{7.1}{154}{Partial Dependence Plot (PDP)}{section.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}PDP-based Feature Importance}{155}{subsection.7.1.1}}
\newlabel{pdp-based-feature-importance}{{7.1.1}{155}{PDP-based Feature Importance}{subsection.7.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Examples}{156}{subsection.7.1.2}}
\newlabel{examples}{{7.1.2}{156}{Examples}{subsection.7.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces PDPs for the bicycle count prediction model and temperature, humidity and wind speed. The largest differences can be seen in the temperature. The hotter, the more bikes are rented. This trend goes up to 20 degrees Celsius, then flattens and drops slightly at 30. Marks on the x-axis indicate the data distribution.}}{157}{figure.7.1}}
\newlabel{fig:pdp-bike}{{7.1}{157}{PDPs for the bicycle count prediction model and temperature, humidity and wind speed. The largest differences can be seen in the temperature. The hotter, the more bikes are rented. This trend goes up to 20 degrees Celsius, then flattens and drops slightly at 30. Marks on the x-axis indicate the data distribution}{figure.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces PDPs for the bike count prediction model and the season. Unexpectedly all seasons show similar effect on the model predictions, only for spring the model predicts fewer bicycle rentals.}}{158}{figure.7.2}}
\newlabel{fig:pdp-bike-cat}{{7.2}{158}{PDPs for the bike count prediction model and the season. Unexpectedly all seasons show similar effect on the model predictions, only for spring the model predicts fewer bicycle rentals}{figure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces PDPs of cancer probability based on age and years with hormonal contraceptives. For age, the PDP shows that the probability is low until 40 and increases after. The more years on hormonal contraceptives the higher the predicted cancer risk, especially after 10 years. For both features not many data points with large values were available, so the PD estimates are less reliable in those regions.}}{159}{figure.7.3}}
\newlabel{fig:pdp-cervical}{{7.3}{159}{PDPs of cancer probability based on age and years with hormonal contraceptives. For age, the PDP shows that the probability is low until 40 and increases after. The more years on hormonal contraceptives the higher the predicted cancer risk, especially after 10 years. For both features not many data points with large values were available, so the PD estimates are less reliable in those regions}{figure.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Advantages}{159}{subsection.7.1.3}}
\newlabel{advantages-5}{{7.1.3}{159}{Advantages}{subsection.7.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces PDP of cancer probability and the interaction of age and number of pregnancies. The plot shows the increase in cancer probability at 45. For ages below 25, women who had 1 or 2 pregnancies have a lower predicted cancer risk, compared with women who had 0 or more than 2 pregnancies. But be careful when drawing conclusions: This might just be a correlation and not causal!}}{160}{figure.7.4}}
\newlabel{fig:pdp-cervical-2d}{{7.4}{160}{PDP of cancer probability and the interaction of age and number of pregnancies. The plot shows the increase in cancer probability at 45. For ages below 25, women who had 1 or 2 pregnancies have a lower predicted cancer risk, compared with women who had 0 or more than 2 pregnancies. But be careful when drawing conclusions: This might just be a correlation and not causal!}{figure.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}Disadvantages}{161}{subsection.7.1.4}}
\newlabel{disadvantages-5}{{7.1.4}{161}{Disadvantages}{subsection.7.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.5}Software and Alternatives}{162}{subsection.7.1.5}}
\newlabel{software-and-alternatives-1}{{7.1.5}{162}{Software and Alternatives}{subsection.7.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Accumulated Local Effects (ALE) Plot}{163}{section.7.2}}
\newlabel{ale}{{7.2}{163}{Accumulated Local Effects (ALE) Plot}{section.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Motivation and Intuition}{163}{subsection.7.2.1}}
\newlabel{motivation-and-intuition}{{7.2.1}{163}{Motivation and Intuition}{subsection.7.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Strongly correlated features x1 and x2. To calculate the feature effect of x1 at 0.75, the PDP replaces x1 of all instances with 0.75, falsely assuming that the distribution of x2 at x1 = 0.75 is the same as the marginal distribution of x2 (vertical line). This results in unlikely combinations of x1 and x2 (e.g. x2=0.2 at x1=0.75), which the PDP uses for the calculation of the average effect.}}{164}{figure.7.5}}
\newlabel{fig:aleplot-motivation1}{{7.5}{164}{Strongly correlated features x1 and x2. To calculate the feature effect of x1 at 0.75, the PDP replaces x1 of all instances with 0.75, falsely assuming that the distribution of x2 at x1 = 0.75 is the same as the marginal distribution of x2 (vertical line). This results in unlikely combinations of x1 and x2 (e.g. x2=0.2 at x1=0.75), which the PDP uses for the calculation of the average effect}{figure.7.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Strongly correlated features x1 and x2. M-Plots average over the conditional distribution. Here the conditional distribution of x2 at x1 = 0.75. Averaging the local predictions leads to mixing the effects of both features.}}{165}{figure.7.6}}
\newlabel{fig:aleplot-motivation2}{{7.6}{165}{Strongly correlated features x1 and x2. M-Plots average over the conditional distribution. Here the conditional distribution of x2 at x1 = 0.75. Averaging the local predictions leads to mixing the effects of both features}{figure.7.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Calculation of ALE for feature x1, which is correlated with x2. First, we divide the feature into intervals (vertical lines). For the data instances (points) in an interval, we calculate the difference in the prediction when we replace the feature with the upper and lower limit of the interval (horizontal lines). These differences are later accumulated and centered, resulting in the ALE curve.}}{166}{figure.7.7}}
\newlabel{fig:aleplot-computation}{{7.7}{166}{Calculation of ALE for feature x1, which is correlated with x2. First, we divide the feature into intervals (vertical lines). For the data instances (points) in an interval, we calculate the difference in the prediction when we replace the feature with the upper and lower limit of the interval (horizontal lines). These differences are later accumulated and centered, resulting in the ALE curve}{figure.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Theory}{167}{subsection.7.2.2}}
\newlabel{theory-2}{{7.2.2}{167}{Theory}{subsection.7.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Estimation}{169}{subsection.7.2.3}}
\newlabel{estimation}{{7.2.3}{169}{Estimation}{subsection.7.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Calculation of 2D-ALE. We place a grid over the two features. In each grid cell we calculate the 2nd-order differences for all instance within. We first replace values of x1 and x2 with the values from the cell corners. If a, b, c and d represent the "corner"-predictions of a manipulated instance (as labeled in the graphic), then the 2nd-order difference is (d - c) - (b - a). The mean 2nd-order difference in each cell is accumulated over the grid and centered.}}{171}{figure.7.8}}
\newlabel{fig:aleplot-computation-2d}{{7.8}{171}{Calculation of 2D-ALE. We place a grid over the two features. In each grid cell we calculate the 2nd-order differences for all instance within. We first replace values of x1 and x2 with the values from the cell corners. If a, b, c and d represent the "corner"-predictions of a manipulated instance (as labeled in the graphic), then the 2nd-order difference is (d - c) - (b - a). The mean 2nd-order difference in each cell is accumulated over the grid and centered}{figure.7.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Examples}{173}{subsection.7.2.4}}
\newlabel{examples-1}{{7.2.4}{173}{Examples}{subsection.7.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Two features and the predicted outcome. The model predicts the sum of the two features (shaded background), with the exception that if x1 is greater than 0.7 and x2 less than 0.3, the model always predicts 2. This area is far from the distribution of data (point cloud) and does not affect the performance of the model and also should not affect its interpretation.}}{174}{figure.7.9}}
\newlabel{fig:correlation-problem}{{7.9}{174}{Two features and the predicted outcome. The model predicts the sum of the two features (shaded background), with the exception that if x1 is greater than 0.7 and x2 less than 0.3, the model always predicts 2. This area is far from the distribution of data (point cloud) and does not affect the performance of the model and also should not affect its interpretation}{figure.7.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Comparison of the feature effects computed with PDP (upper row) and ALE (lower row). The PDP estimates are influenced by the odd behavior of the model outside the data distribution (steep jumps in the plots). The ALE plots correctly identify that the machine learning model has a linear relationship between features and prediction, ignoring areas without data.}}{175}{figure.7.10}}
\newlabel{fig:correlation-pdp-ale-plot}{{7.10}{175}{Comparison of the feature effects computed with PDP (upper row) and ALE (lower row). The PDP estimates are influenced by the odd behavior of the model outside the data distribution (steep jumps in the plots). The ALE plots correctly identify that the machine learning model has a linear relationship between features and prediction, ignoring areas without data}{figure.7.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces ALE plots for the bike prediction model by temperature, humidity and wind speed. The temperature has a strong effect on the prediction. The average prediction rises with increasing temperature, but falls again above 25 degrees Celsius. Humidity has a negative effect: When above 60\%, the higher the relative humidity, the lower the prediction. The wind speed does not affect the predictions much.}}{176}{figure.7.11}}
\newlabel{fig:ale-bike}{{7.11}{176}{ALE plots for the bike prediction model by temperature, humidity and wind speed. The temperature has a strong effect on the prediction. The average prediction rises with increasing temperature, but falls again above 25 degrees Celsius. Humidity has a negative effect: When above 60\%, the higher the relative humidity, the lower the prediction. The wind speed does not affect the predictions much}{figure.7.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces The strength of the correlation between temperature, humidity and wind speed with all features, measured as the amount of variance explained, when we train a linear model with e.g. temperature to predict and season as feature. For temperature we observe -- not surprisingly -- a high correlation with season and month. Humidity correlates with weather situation.}}{177}{figure.7.12}}
\newlabel{fig:ale-bike-cor}{{7.12}{177}{The strength of the correlation between temperature, humidity and wind speed with all features, measured as the amount of variance explained, when we train a linear model with e.g. temperature to predict and season as feature. For temperature we observe -- not surprisingly -- a high correlation with season and month. Humidity correlates with weather situation}{figure.7.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces PDPs for temperature, humidity and wind speed. Compared to the ALE plots, the PDPs show a smaller decrease in predicted number of bikes for high temperature or high humidity. The PDP uses all data instances to calculate the effect of high temperatures, even if they are, for example, instances with the season "winter". The ALE plots are more reliable.}}{178}{figure.7.13}}
\newlabel{fig:pdp-bike-compare}{{7.13}{178}{PDPs for temperature, humidity and wind speed. Compared to the ALE plots, the PDPs show a smaller decrease in predicted number of bikes for high temperature or high humidity. The PDP uses all data instances to calculate the effect of high temperatures, even if they are, for example, instances with the season "winter". The ALE plots are more reliable}{figure.7.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces ALE plot for the categorical feature month. The months are ordered by their similarity to each other, based on the distributions of the other features by month. We observe that January, March and April, but especially December and November, have a lower effect on the predicted number of rented bikes compared to the other months.}}{179}{figure.7.14}}
\newlabel{fig:ale-bike-cat}{{7.14}{179}{ALE plot for the categorical feature month. The months are ordered by their similarity to each other, based on the distributions of the other features by month. We observe that January, March and April, but especially December and November, have a lower effect on the predicted number of rented bikes compared to the other months}{figure.7.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces ALE plot for the 2nd-order effect of humidity and temperature on the predicted number of rented bikes. Lighter shade indicates an above average and darker shade a below average prediction when the main effects are already taken into account. The plot reveals an interaction between temperature and humidity: Hot and humid weather increases the prediction. In cold and humid weather an additional negative effect on the number of predicted bikes is shown.}}{180}{figure.7.15}}
\newlabel{fig:ale-bike-2d}{{7.15}{180}{ALE plot for the 2nd-order effect of humidity and temperature on the predicted number of rented bikes. Lighter shade indicates an above average and darker shade a below average prediction when the main effects are already taken into account. The plot reveals an interaction between temperature and humidity: Hot and humid weather increases the prediction. In cold and humid weather an additional negative effect on the number of predicted bikes is shown}{figure.7.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.16}{\ignorespaces PDP of the total effect of temperature and humidity on the predicted number of bikes. The plot combines the main effect of each of the features and their interaction effect, as opposed to the 2D-ALE plot which only shows the interaction.}}{181}{figure.7.16}}
\newlabel{fig:pdp-bike-vs-ale-2D}{{7.16}{181}{PDP of the total effect of temperature and humidity on the predicted number of bikes. The plot combines the main effect of each of the features and their interaction effect, as opposed to the 2D-ALE plot which only shows the interaction}{figure.7.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.5}Advantages}{181}{subsection.7.2.5}}
\newlabel{advantages-6}{{7.2.5}{181}{Advantages}{subsection.7.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.17}{\ignorespaces ALE plots for the effect of age and years with hormonal contraceptives on the predicted probability of cervical cancer. For the age feature, the ALE plot shows that the predicted cancer probability is low on average up to age 40 and increases after that. The number of years with hormonal contraceptives is associated with a higher predicted cancer risk after 8 years.}}{182}{figure.7.17}}
\newlabel{fig:ale-cervical-1D}{{7.17}{182}{ALE plots for the effect of age and years with hormonal contraceptives on the predicted probability of cervical cancer. For the age feature, the ALE plot shows that the predicted cancer probability is low on average up to age 40 and increases after that. The number of years with hormonal contraceptives is associated with a higher predicted cancer risk after 8 years}{figure.7.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.18}{\ignorespaces ALE plot of the 2nd-order effect of number of pregnancies and age. The interpretation of the plot is a bit inconclusive, showing what seems like overfitting. For example, the plot shows an odd model behavior at age of 18-20 and more than 3 pregnancies (up to 5 percentage point increase in cancer probability). There are not many women in the data with this constellation of age and number of pregnancies (actual data are displayed as points), so the model is not severely penalized during the training for making mistakes for those women.}}{183}{figure.7.18}}
\newlabel{fig:ale-cervical-2d}{{7.18}{183}{ALE plot of the 2nd-order effect of number of pregnancies and age. The interpretation of the plot is a bit inconclusive, showing what seems like overfitting. For example, the plot shows an odd model behavior at age of 18-20 and more than 3 pregnancies (up to 5 percentage point increase in cancer probability). There are not many women in the data with this constellation of age and number of pregnancies (actual data are displayed as points), so the model is not severely penalized during the training for making mistakes for those women}{figure.7.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.6}Disadvantages}{183}{subsection.7.2.6}}
\newlabel{disadvantages-6}{{7.2.6}{183}{Disadvantages}{subsection.7.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.7}Implementation and Alternatives}{185}{subsection.7.2.7}}
\newlabel{implementation-and-alternatives}{{7.2.7}{185}{Implementation and Alternatives}{subsection.7.2.7}{}}
\gdef \LT@ii {\LT@entry 
    {1}{47.804pt}\LT@entry 
    {2}{36.256pt}\LT@entry 
    {1}{56.564pt}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Feature Interaction}{186}{section.7.3}}
\newlabel{interaction}{{7.3}{186}{Feature Interaction}{section.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Feature Interaction?}{186}{subsection.7.3.1}}
\newlabel{feature-interaction}{{7.3.1}{186}{Feature Interaction?}{subsection.7.3.1}{}}
\gdef \LT@iii {\LT@entry 
    {1}{47.804pt}\LT@entry 
    {2}{36.256pt}\LT@entry 
    {1}{56.564pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Theory: Friedman's H-statistic}{187}{subsection.7.3.2}}
\newlabel{theory-friedmans-h-statistic}{{7.3.2}{187}{Theory: Friedman's H-statistic}{subsection.7.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Examples}{190}{subsection.7.3.3}}
\newlabel{examples-2}{{7.3.3}{190}{Examples}{subsection.7.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.19}{\ignorespaces The interaction strength (H-statistic) for each feature with all other features for a support vector machine predicting bicycle rentals. Overall, the interaction effects between the features are very weak (below 10\% of variance explained per feature).}}{190}{figure.7.19}}
\newlabel{fig:interaction-bike}{{7.19}{190}{The interaction strength (H-statistic) for each feature with all other features for a support vector machine predicting bicycle rentals. Overall, the interaction effects between the features are very weak (below 10\% of variance explained per feature)}{figure.7.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.20}{\ignorespaces The interaction strength (H-statistic) for each feature with all other features for a random forest predicting the probability of cervical cancer. The years on hormonal contraceptives has the highest relative interaction effect with all other features, followed by the number of pregnancies.}}{191}{figure.7.20}}
\newlabel{fig:interaction-cervical-include}{{7.20}{191}{The interaction strength (H-statistic) for each feature with all other features for a random forest predicting the probability of cervical cancer. The years on hormonal contraceptives has the highest relative interaction effect with all other features, followed by the number of pregnancies}{figure.7.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Advantages}{191}{subsection.7.3.4}}
\newlabel{advantages-7}{{7.3.4}{191}{Advantages}{subsection.7.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.21}{\ignorespaces The 2-way interaction strengths (H-statistic) between number of pregnancies and each other feature. There is a strong interaction between the number of pregnancies and the age.}}{192}{figure.7.21}}
\newlabel{fig:interaction2-cervical-age-include}{{7.21}{192}{The 2-way interaction strengths (H-statistic) between number of pregnancies and each other feature. There is a strong interaction between the number of pregnancies and the age}{figure.7.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.5}Disadvantages}{192}{subsection.7.3.5}}
\newlabel{disadvantages-7}{{7.3.5}{192}{Disadvantages}{subsection.7.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.6}Implementations}{193}{subsection.7.3.6}}
\newlabel{implementations}{{7.3.6}{193}{Implementations}{subsection.7.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.7}Alternatives}{194}{subsection.7.3.7}}
\newlabel{alternatives}{{7.3.7}{194}{Alternatives}{subsection.7.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Functional Decompositon}{194}{section.7.4}}
\newlabel{decomposition}{{7.4}{194}{Functional Decompositon}{section.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.22}{\ignorespaces Prediction surface of a function with two features $X_1$ and $X_2$.}}{195}{figure.7.22}}
\newlabel{fig:unnamed-chunk-16}{{7.22}{195}{Prediction surface of a function with two features $X_1$ and $X_2$}{figure.7.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}How not to Compute the Components I}{196}{subsection.7.4.1}}
\newlabel{how-not-to-compute-the-components-i}{{7.4.1}{196}{How not to Compute the Components I}{subsection.7.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.23}{\ignorespaces Decomposition of a function.}}{197}{figure.7.23}}
\newlabel{fig:unnamed-chunk-17}{{7.23}{197}{Decomposition of a function}{figure.7.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Functional Decomposition}{198}{subsection.7.4.2}}
\newlabel{functional-decomposition}{{7.4.2}{198}{Functional Decomposition}{subsection.7.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}How not to Compute the Components II}{199}{subsection.7.4.3}}
\newlabel{how-not-to-compute-the-components-ii}{{7.4.3}{199}{How not to Compute the Components II}{subsection.7.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.4}Functional ANOVA}{200}{subsection.7.4.4}}
\newlabel{functional-anova}{{7.4.4}{200}{Functional ANOVA}{subsection.7.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.5}Generalized Functional ANOVA for Dependent Features}{202}{subsection.7.4.5}}
\newlabel{generalized-functional-anova-for-dependent-features}{{7.4.5}{202}{Generalized Functional ANOVA for Dependent Features}{subsection.7.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.6}Accumulated Local Effect Plots}{204}{subsection.7.4.6}}
\newlabel{accumulated-local-effect-plots}{{7.4.6}{204}{Accumulated Local Effect Plots}{subsection.7.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.7}Statistical Regression Models}{205}{subsection.7.4.7}}
\newlabel{statistical-regression-models}{{7.4.7}{205}{Statistical Regression Models}{subsection.7.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.8}Bonus: Partial Dependence Plot}{206}{subsection.7.4.8}}
\newlabel{bonus-partial-dependence-plot}{{7.4.8}{206}{Bonus: Partial Dependence Plot}{subsection.7.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.9}Advantages}{207}{subsection.7.4.9}}
\newlabel{advantages-8}{{7.4.9}{207}{Advantages}{subsection.7.4.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.10}Disadvantages}{207}{subsection.7.4.10}}
\newlabel{disadvantages-8}{{7.4.10}{207}{Disadvantages}{subsection.7.4.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Permutation Feature Importance}{209}{section.7.5}}
\newlabel{feature-importance}{{7.5}{209}{Permutation Feature Importance}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Theory}{209}{subsection.7.5.1}}
\newlabel{theory-3}{{7.5.1}{209}{Theory}{subsection.7.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Should I Compute Importance on Training or Test Data?}{210}{subsection.7.5.2}}
\newlabel{feature-importance-data}{{7.5.2}{210}{Should I Compute Importance on Training or Test Data?}{subsection.7.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.24}{\ignorespaces Distributions of feature importance values by data type. An SVM was trained on a regression dataset with 50 random features and 200 instances. The SVM overfits the data: Feature importance based on the training data shows many important features. Computed on unseen test data, the feature importances are close to a ratio of one (=unimportant).}}{211}{figure.7.24}}
\newlabel{fig:feature-imp-sim}{{7.24}{211}{Distributions of feature importance values by data type. An SVM was trained on a regression dataset with 50 random features and 200 instances. The SVM overfits the data: Feature importance based on the training data shows many important features. Computed on unseen test data, the feature importances are close to a ratio of one (=unimportant)}{figure.7.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.25}{\ignorespaces PDP of feature X42, which is the most important feature according to the feature importance based on the training data. The plot shows how the SVM depends on this feature to make predictions}}{213}{figure.7.25}}
\newlabel{fig:garbage-svm-pdp}{{7.25}{213}{PDP of feature X42, which is the most important feature according to the feature importance based on the training data. The plot shows how the SVM depends on this feature to make predictions}{figure.7.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Example and Interpretation}{214}{subsection.7.5.3}}
\newlabel{example-and-interpretation}{{7.5.3}{214}{Example and Interpretation}{subsection.7.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.26}{\ignorespaces The importance of each of the features for predicting cervical cancer with a random forest. The most important feature was Age. Permuting Age resulted in an increase in 1-AUC by a factor of 6.49}}{214}{figure.7.26}}
\newlabel{fig:importance-cervical}{{7.26}{214}{The importance of each of the features for predicting cervical cancer with a random forest. The most important feature was Age. Permuting Age resulted in an increase in 1-AUC by a factor of 6.49}{figure.7.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.27}{\ignorespaces The importance for each of the features in predicting bike counts with a support vector machine. The most important feature was temp, the least important was holiday.}}{215}{figure.7.27}}
\newlabel{fig:importance-bike}{{7.27}{215}{The importance for each of the features in predicting bike counts with a support vector machine. The most important feature was temp, the least important was holiday}{figure.7.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Advantages}{215}{subsection.7.5.4}}
\newlabel{advantages-9}{{7.5.4}{215}{Advantages}{subsection.7.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.5}Disadvantages}{217}{subsection.7.5.5}}
\newlabel{disadvantages-9}{{7.5.5}{217}{Disadvantages}{subsection.7.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.6}Software and Alternatives}{219}{subsection.7.5.6}}
\newlabel{software-and-alternatives-2}{{7.5.6}{219}{Software and Alternatives}{subsection.7.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Global Surrogate}{220}{section.7.6}}
\newlabel{global}{{7.6}{220}{Global Surrogate}{section.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}Theory}{220}{subsection.7.6.1}}
\newlabel{theory-4}{{7.6.1}{220}{Theory}{subsection.7.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Example}{222}{subsection.7.6.2}}
\newlabel{example-3}{{7.6.2}{222}{Example}{subsection.7.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.28}{\ignorespaces The terminal nodes of a surrogate tree that approximates the predictions of a support vector machine trained on the bike rental dataset. The distributions in the nodes show that the surrogate tree predicts a higher number of rented bikes when temperature is above 13 degrees Celsius and when the day was later in the 2 year period (cut point at 435 days).}}{223}{figure.7.28}}
\newlabel{fig:surrogate-bike}{{7.28}{223}{The terminal nodes of a surrogate tree that approximates the predictions of a support vector machine trained on the bike rental dataset. The distributions in the nodes show that the surrogate tree predicts a higher number of rented bikes when temperature is above 13 degrees Celsius and when the day was later in the 2 year period (cut point at 435 days)}{figure.7.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.29}{\ignorespaces The terminal nodes of a surrogate tree that approximates the predictions of a random forest trained on the cervical cancer dataset. The counts in the nodes show the frequency of the black box models classifications in the nodes.}}{224}{figure.7.29}}
\newlabel{fig:surrogate-cervical}{{7.29}{224}{The terminal nodes of a surrogate tree that approximates the predictions of a random forest trained on the cervical cancer dataset. The counts in the nodes show the frequency of the black box models classifications in the nodes}{figure.7.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}Advantages}{224}{subsection.7.6.3}}
\newlabel{advantages-10}{{7.6.3}{224}{Advantages}{subsection.7.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.4}Disadvantages}{225}{subsection.7.6.4}}
\newlabel{disadvantages-10}{{7.6.4}{225}{Disadvantages}{subsection.7.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.5}Software}{226}{subsection.7.6.5}}
\newlabel{software-3}{{7.6.5}{226}{Software}{subsection.7.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Prototypes and Criticisms}{227}{section.7.7}}
\newlabel{proto}{{7.7}{227}{Prototypes and Criticisms}{section.7.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.30}{\ignorespaces Prototypes and criticisms for a data distribution with two features x1 and x2.}}{228}{figure.7.30}}
\newlabel{fig:unnamed-chunk-23}{{7.30}{228}{Prototypes and criticisms for a data distribution with two features x1 and x2}{figure.7.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Theory}{228}{subsection.7.7.1}}
\newlabel{theory-5}{{7.7.1}{228}{Theory}{subsection.7.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.31}{\ignorespaces The squared maximum mean discrepancy measure (MMD2) for a dataset with two features and different selections of prototypes.}}{231}{figure.7.31}}
\newlabel{fig:mmd}{{7.31}{231}{The squared maximum mean discrepancy measure (MMD2) for a dataset with two features and different selections of prototypes}{figure.7.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.32}{\ignorespaces Evaluations of the witness function at different points.}}{232}{figure.7.32}}
\newlabel{fig:witness}{{7.32}{232}{Evaluations of the witness function at different points}{figure.7.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Examples}{234}{subsection.7.7.2}}
\newlabel{examples-3}{{7.7.2}{234}{Examples}{subsection.7.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.33}{\ignorespaces Prototypes for a handwritten digits dataset.}}{235}{figure.7.33}}
\newlabel{fig:prototypes-and-criticisms2}{{7.33}{235}{Prototypes for a handwritten digits dataset}{figure.7.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Advantages}{235}{subsection.7.7.3}}
\newlabel{advantages-11}{{7.7.3}{235}{Advantages}{subsection.7.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.4}Disadvantages}{236}{subsection.7.7.4}}
\newlabel{disadvantages-11}{{7.7.4}{236}{Disadvantages}{subsection.7.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.5}Code and Alternatives}{237}{subsection.7.7.5}}
\newlabel{code-and-alternatives}{{7.7.5}{237}{Code and Alternatives}{subsection.7.7.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Local Model-Agnostic Methods}{239}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{local-methods}{{8}{239}{Local Model-Agnostic Methods}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Individual Conditional Expectation (ICE)}{240}{section.8.1}}
\newlabel{ice}{{8.1}{240}{Individual Conditional Expectation (ICE)}{section.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Examples}{241}{subsection.8.1.1}}
\newlabel{examples-4}{{8.1.1}{241}{Examples}{subsection.8.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces ICE plot of cervical cancer probability by age. Each line represents one woman. For most women there is an increase in predicted cancer probability with increasing age. For some women with a predicted cancer probability above 0.4, the prediction does not change much at higher age.}}{241}{figure.8.1}}
\newlabel{fig:ice-cervical}{{8.1}{241}{ICE plot of cervical cancer probability by age. Each line represents one woman. For most women there is an increase in predicted cancer probability with increasing age. For some women with a predicted cancer probability above 0.4, the prediction does not change much at higher age}{figure.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces ICE plots of predicted bicycle rentals by weather conditions. The same effects can be observed as in the partial dependence plots.}}{242}{figure.8.2}}
\newlabel{fig:ice-bike}{{8.2}{242}{ICE plots of predicted bicycle rentals by weather conditions. The same effects can be observed as in the partial dependence plots}{figure.8.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1.1}Centered ICE Plot}{242}{subsubsection.8.1.1.1}}
\newlabel{centered-ice-plot}{{8.1.1.1}{242}{Centered ICE Plot}{subsubsection.8.1.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1.2}Example}{243}{subsubsection.8.1.1.2}}
\newlabel{example-4}{{8.1.1.2}{243}{Example}{subsubsection.8.1.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Centered ICE plot for predicted cancer probability by age. Lines are fixed to 0 at age 14. Compared to age 14, the predictions for most women remain unchanged until the age of 45 where the predicted probability increases.}}{243}{figure.8.3}}
\newlabel{fig:ice-cervical-centered}{{8.3}{243}{Centered ICE plot for predicted cancer probability by age. Lines are fixed to 0 at age 14. Compared to age 14, the predictions for most women remain unchanged until the age of 45 where the predicted probability increases}{figure.8.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Centered ICE plots of predicted number of bikes by weather condition. The lines show the difference in prediction compared to the prediction with the respective feature value at its observed minimum.}}{244}{figure.8.4}}
\newlabel{fig:ice-bike-centered}{{8.4}{244}{Centered ICE plots of predicted number of bikes by weather condition. The lines show the difference in prediction compared to the prediction with the respective feature value at its observed minimum}{figure.8.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1.3}Derivative ICE Plot}{244}{subsubsection.8.1.1.3}}
\newlabel{derivative-ice-plot}{{8.1.1.3}{244}{Derivative ICE Plot}{subsubsection.8.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Advantages}{245}{subsection.8.1.2}}
\newlabel{advantages-12}{{8.1.2}{245}{Advantages}{subsection.8.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Disadvantages}{245}{subsection.8.1.3}}
\newlabel{disadvantages-12}{{8.1.3}{245}{Disadvantages}{subsection.8.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}Software and Alternatives}{246}{subsection.8.1.4}}
\newlabel{software-and-alternatives-3}{{8.1.4}{246}{Software and Alternatives}{subsection.8.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Local Surrogate (LIME)}{247}{section.8.2}}
\newlabel{lime}{{8.2}{247}{Local Surrogate (LIME)}{section.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}LIME for Tabular Data}{249}{subsection.8.2.1}}
\newlabel{lime-for-tabular-data}{{8.2.1}{249}{LIME for Tabular Data}{subsection.8.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces LIME algorithm for tabular data. A) Random forest predictions given features x1 and x2. Predicted classes: 1 (dark) or 0 (light). B) Instance of interest (big dot) and data sampled from a normal distribution (small dots). C) Assign higher weight to points near the instance of interest. D) Signs of the grid show the classifications of the locally learned model from the weighted samples. The white line marks the decision boundary (P(class=1) = 0.5).}}{250}{figure.8.5}}
\newlabel{fig:lime-fitting}{{8.5}{250}{LIME algorithm for tabular data. A) Random forest predictions given features x1 and x2. Predicted classes: 1 (dark) or 0 (light). B) Instance of interest (big dot) and data sampled from a normal distribution (small dots). C) Assign higher weight to points near the instance of interest. D) Signs of the grid show the classifications of the locally learned model from the weighted samples. The white line marks the decision boundary (P(class=1) = 0.5)}{figure.8.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Explanation of the prediction of instance x = 1.6. The predictions of the black box model depending on a single feature is shown as a thick line and the distribution of the data is shown with rugs. Three local surrogate models with different kernel widths are computed. The resulting linear regression model depends on the kernel width: Does the feature have a negative, positive or no effect for x = 1.6?}}{251}{figure.8.6}}
\newlabel{fig:lime-fail}{{8.6}{251}{Explanation of the prediction of instance x = 1.6. The predictions of the black box model depending on a single feature is shown as a thick line and the distribution of the data is shown with rugs. Three local surrogate models with different kernel widths are computed. The resulting linear regression model depends on the kernel width: Does the feature have a negative, positive or no effect for x = 1.6?}{figure.8.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1.1}Example}{252}{subsubsection.8.2.1.1}}
\newlabel{example-5}{{8.2.1.1}{252}{Example}{subsubsection.8.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}LIME for Text}{252}{subsection.8.2.2}}
\newlabel{lime-for-text}{{8.2.2}{252}{LIME for Text}{subsection.8.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2.1}Example}{252}{subsubsection.8.2.2.1}}
\newlabel{example-6}{{8.2.2.1}{252}{Example}{subsubsection.8.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces LIME explanations for two instances of the bike rental dataset. Warmer temperature and good weather situation have a positive effect on the prediction. The x-axis shows the feature effect: The weight times the actual feature value.}}{253}{figure.8.7}}
\newlabel{fig:lime-tabular-example-explain-plot-1}{{8.7}{253}{LIME explanations for two instances of the bike rental dataset. Warmer temperature and good weather situation have a positive effect on the prediction. The x-axis shows the feature effect: The weight times the actual feature value}{figure.8.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}LIME for Images}{254}{subsection.8.2.3}}
\newlabel{images-lime}{{8.2.3}{254}{LIME for Images}{subsection.8.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3.1}Example}{255}{subsubsection.8.2.3.1}}
\newlabel{example-7}{{8.2.3.1}{255}{Example}{subsubsection.8.2.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Left: Image of a bowl of bread. Middle and right: LIME explanations for the top 2 classes (bagel, strawberry) for image classification made by Google's Inception V3 neural network.}}{255}{figure.8.8}}
\newlabel{fig:lime-images-package-example-include}{{8.8}{255}{Left: Image of a bowl of bread. Middle and right: LIME explanations for the top 2 classes (bagel, strawberry) for image classification made by Google's Inception V3 neural network}{figure.8.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}Advantages}{256}{subsection.8.2.4}}
\newlabel{advantages-13}{{8.2.4}{256}{Advantages}{subsection.8.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.5}Disadvantages}{257}{subsection.8.2.5}}
\newlabel{disadvantages-13}{{8.2.5}{257}{Disadvantages}{subsection.8.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Counterfactual Explanations}{259}{section.8.3}}
\newlabel{counterfactual}{{8.3}{259}{Counterfactual Explanations}{section.8.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces The causal relationships between inputs of a machine learning model and the predictions, when the model is merely seen as a black box. The inputs cause the prediction (not necessarily reflecting the real causal relation of the data).}}{260}{figure.8.9}}
\newlabel{fig:ml-graph-cf}{{8.9}{260}{The causal relationships between inputs of a machine learning model and the predictions, when the model is merely seen as a black box. The inputs cause the prediction (not necessarily reflecting the real causal relation of the data)}{figure.8.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Generating Counterfactual Explanations}{263}{subsection.8.3.1}}
\newlabel{generating-counterfactual-explanations}{{8.3.1}{263}{Generating Counterfactual Explanations}{subsection.8.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1.1}Method by Wachter et al.}{264}{subsubsection.8.3.1.1}}
\newlabel{method-by-wachter-et-al.}{{8.3.1.1}{264}{Method by Wachter et al}{subsubsection.8.3.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1.2}Method by Dandl et al.}{266}{subsubsection.8.3.1.2}}
\newlabel{method-by-dandl-et-al.}{{8.3.1.2}{266}{Method by Dandl et al}{subsubsection.8.3.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Visualization of one generation of the NSGA-II algorithm.}}{268}{figure.8.10}}
\newlabel{fig:nsgaII-cf}{{8.10}{268}{Visualization of one generation of the NSGA-II algorithm}{figure.8.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Example}{269}{subsection.8.3.2}}
\newlabel{example-8}{{8.3.2}{269}{Example}{subsection.8.3.2}{}}
\gdef \LT@iv {\LT@entry 
    {1}{22.016pt}\LT@entry 
    {1}{25.9pt}\LT@entry 
    {2}{55.084pt}\LT@entry 
    {1}{49.876pt}\LT@entry 
    {1}{46.756pt}\LT@entry 
    {1}{48.748pt}\LT@entry 
    {1}{53.188pt}\LT@entry 
    {1}{44.936pt}}
\gdef \LT@v {\LT@entry 
    {1}{22.016pt}\LT@entry 
    {1}{25.9pt}\LT@entry 
    {2}{41.848pt}\LT@entry 
    {1}{48.748pt}\LT@entry 
    {1}{53.188pt}\LT@entry 
    {2}{37.072pt}\LT@entry 
    {1}{20.39853pt}\LT@entry 
    {2}{37.756pt}\LT@entry 
    {2}{31.148pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Advantages}{271}{subsection.8.3.3}}
\newlabel{advantages-14}{{8.3.3}{271}{Advantages}{subsection.8.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Disadvantages}{272}{subsection.8.3.4}}
\newlabel{disadvantages-14}{{8.3.4}{272}{Disadvantages}{subsection.8.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Software and Alternatives}{272}{subsection.8.3.5}}
\newlabel{example-software}{{8.3.5}{272}{Software and Alternatives}{subsection.8.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Scoped Rules (Anchors)}{274}{section.8.4}}
\newlabel{anchors}{{8.4}{274}{Scoped Rules (Anchors)}{section.8.4}{}}
\gdef \LT@vi {\LT@entry 
    {3}{81.776pt}\LT@entry 
    {3}{38.216pt}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces LIME vs. Anchors -- A Toy Visualization. Figure from Ribeiro, Singh, and Guestrin (2018).}}{275}{figure.8.11}}
\newlabel{fig:unnamed-chunk-29}{{8.11}{275}{LIME vs. Anchors -- A Toy Visualization. Figure from Ribeiro, Singh, and Guestrin (2018)}{figure.8.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.3}{one exemplary individual and the model's prediction}}{275}{table.8.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Finding Anchors}{277}{subsection.8.4.1}}
\newlabel{finding-anchors}{{8.4.1}{277}{Finding Anchors}{subsection.8.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces The anchors algorithm’s components and their interrelations (simplified)}}{279}{figure.8.12}}
\newlabel{fig:unnamed-chunk-30}{{8.12}{279}{The anchors algorithm’s components and their interrelations (simplified)}{figure.8.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}Complexity and Runtime}{280}{subsection.8.4.2}}
\newlabel{complexity-and-runtime}{{8.4.2}{280}{Complexity and Runtime}{subsection.8.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}Tabular Data Example}{280}{subsection.8.4.3}}
\newlabel{tabular-data-example}{{8.4.3}{280}{Tabular Data Example}{subsection.8.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces Anchors explaining six instances of the bike rental dataset. Each row represents one explanation or anchor, and each bar depicts the feature predicates contained by it. The x-axis displays a rule's precision, and a bar's thickness corresponds to its coverage. The 'base' rule contains no predicates. These anchors show that the model mainly considers the temperature for predictions.}}{281}{figure.8.13}}
\newlabel{fig:unnamed-chunk-33}{{8.13}{281}{Anchors explaining six instances of the bike rental dataset. Each row represents one explanation or anchor, and each bar depicts the feature predicates contained by it. The x-axis displays a rule's precision, and a bar's thickness corresponds to its coverage. The 'base' rule contains no predicates. These anchors show that the model mainly considers the temperature for predictions}{figure.8.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces Explaining instances near decision boundaries leads to specific rules comprising a higher number of feature predicates and lower coverage. Also, the empty rule, i.e., the base feature, gets less important. This can be interpreted as a signal for a decision boundary, as the instance is located in a volatile neighborhood.}}{282}{figure.8.14}}
\newlabel{fig:unnamed-chunk-34}{{8.14}{282}{Explaining instances near decision boundaries leads to specific rules comprising a higher number of feature predicates and lower coverage. Also, the empty rule, i.e., the base feature, gets less important. This can be interpreted as a signal for a decision boundary, as the instance is located in a volatile neighborhood}{figure.8.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Constructing anchors within unbalanced perturbation spaces leads to unexpressive results.}}{283}{figure.8.15}}
\newlabel{fig:unnamed-chunk-37}{{8.15}{283}{Constructing anchors within unbalanced perturbation spaces leads to unexpressive results}{figure.8.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces Balancing the data set before constructing anchors shows the model's reasoning for decisions in minority cases.}}{284}{figure.8.16}}
\newlabel{fig:unnamed-chunk-38}{{8.16}{284}{Balancing the data set before constructing anchors shows the model's reasoning for decisions in minority cases}{figure.8.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.4}Advantages}{285}{subsection.8.4.4}}
\newlabel{advantages-15}{{8.4.4}{285}{Advantages}{subsection.8.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.5}Disadvantages}{285}{subsection.8.4.5}}
\newlabel{disadvantages-15}{{8.4.5}{285}{Disadvantages}{subsection.8.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.6}Software and Alternatives}{286}{subsection.8.4.6}}
\newlabel{software-and-alternatives-4}{{8.4.6}{286}{Software and Alternatives}{subsection.8.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Shapley Values}{287}{section.8.5}}
\newlabel{shapley}{{8.5}{287}{Shapley Values}{section.8.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}General Idea}{287}{subsection.8.5.1}}
\newlabel{general-idea}{{8.5.1}{287}{General Idea}{subsection.8.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces The predicted price for a 50 $m^2$ 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction.}}{287}{figure.8.17}}
\newlabel{fig:shapley-instance}{{8.17}{287}{The predicted price for a 50 $m^2$ 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction}{figure.8.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces One sample repetition to estimate the contribution of `cat-banned` to the prediction when added to the coalition of `park-nearby` and `area-50`.}}{289}{figure.8.18}}
\newlabel{fig:shapley-instance-intervened}{{8.18}{289}{One sample repetition to estimate the contribution of `cat-banned` to the prediction when added to the coalition of `park-nearby` and `area-50`}{figure.8.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}Examples and Interpretation}{290}{subsection.8.5.2}}
\newlabel{examples-and-interpretation}{{8.5.2}{290}{Examples and Interpretation}{subsection.8.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.19}{\ignorespaces All 8 coalitions needed for computing the exact Shapley value of the `cat-banned` feature value.}}{291}{figure.8.19}}
\newlabel{fig:shapley-coalitions}{{8.19}{291}{All 8 coalitions needed for computing the exact Shapley value of the `cat-banned` feature value}{figure.8.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}The Shapley Value in Detail}{291}{subsection.8.5.3}}
\newlabel{the-shapley-value-in-detail}{{8.5.3}{291}{The Shapley Value in Detail}{subsection.8.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.20}{\ignorespaces Shapley values for a woman in the cervical cancer dataset. With a prediction of 0.53, this woman's cancer probability is 0.51 above the average prediction of 0.03. The number of diagnosed STDs increased the probability the most. The sum of contributions yields the difference between actual and average prediction (0.51).}}{292}{figure.8.20}}
\newlabel{fig:shapley-cervical-plot}{{8.20}{292}{Shapley values for a woman in the cervical cancer dataset. With a prediction of 0.53, this woman's cancer probability is 0.51 above the average prediction of 0.03. The number of diagnosed STDs increased the probability the most. The sum of contributions yields the difference between actual and average prediction (0.51)}{figure.8.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.21}{\ignorespaces Shapley values for day 285. With a predicted 2475 rental bikes, this day is -2041 below the average prediction of 4516. The weather situation and humidity had the largest negative contributions. The temperature on this day had a positive contribution. The sum of Shapley values yields the difference of actual and average prediction (-2041).}}{293}{figure.8.21}}
\newlabel{fig:shapley-bike-plot}{{8.21}{293}{Shapley values for day 285. With a predicted 2475 rental bikes, this day is -2041 below the average prediction of 4516. The weather situation and humidity had the largest negative contributions. The temperature on this day had a positive contribution. The sum of Shapley values yields the difference of actual and average prediction (-2041)}{figure.8.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.3.1}The Shapley Value}{294}{subsubsection.8.5.3.1}}
\newlabel{the-shapley-value}{{8.5.3.1}{294}{The Shapley Value}{subsubsection.8.5.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.3.2}Intuition}{296}{subsubsection.8.5.3.2}}
\newlabel{intuition}{{8.5.3.2}{296}{Intuition}{subsubsection.8.5.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.3.3}Estimating the Shapley Value}{296}{subsubsection.8.5.3.3}}
\newlabel{estimating-the-shapley-value}{{8.5.3.3}{296}{Estimating the Shapley Value}{subsubsection.8.5.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Advantages}{298}{subsection.8.5.4}}
\newlabel{advantages-16}{{8.5.4}{298}{Advantages}{subsection.8.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.5}Disadvantages}{299}{subsection.8.5.5}}
\newlabel{disadvantages-16}{{8.5.5}{299}{Disadvantages}{subsection.8.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.6}Software and Alternatives}{301}{subsection.8.5.6}}
\newlabel{software-and-alternatives-5}{{8.5.6}{301}{Software and Alternatives}{subsection.8.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}SHAP (SHapley Additive exPlanations)}{302}{section.8.6}}
\newlabel{shap}{{8.6}{302}{SHAP (SHapley Additive exPlanations)}{section.8.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.1}Definition}{302}{subsection.8.6.1}}
\newlabel{definition}{{8.6.1}{302}{Definition}{subsection.8.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.2}KernelSHAP}{305}{subsection.8.6.2}}
\newlabel{kernelshap}{{8.6.2}{305}{KernelSHAP}{subsection.8.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.22}{\ignorespaces Function $h_x$ maps a coalition to a valid instance. For present features (1), $h_x$ maps to the feature values of x. For absent features (0), $h_x$ maps to the values of a randomly sampled data instance.}}{306}{figure.8.22}}
\newlabel{fig:shap-simplified-feature}{{8.22}{306}{Function $h_x$ maps a coalition to a valid instance. For present features (1), $h_x$ maps to the feature values of x. For absent features (0), $h_x$ maps to the values of a randomly sampled data instance}{figure.8.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.23}{\ignorespaces Function $h_x$ maps coalitions of super pixels (sp) to images. Super-pixels are groups of pixels. For present features (1), $h_x$ returns the corresponding part of the original image. For absent features (0), $h_x$ greys out the corresponding area. Assigning the average color of surrounding pixels or similar would also be an option.}}{307}{figure.8.23}}
\newlabel{fig:unnamed-chunk-43}{{8.23}{307}{Function $h_x$ maps coalitions of super pixels (sp) to images. Super-pixels are groups of pixels. For present features (1), $h_x$ returns the corresponding part of the original image. For absent features (0), $h_x$ greys out the corresponding area. Assigning the average color of surrounding pixels or similar would also be an option}{figure.8.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.3}TreeSHAP}{309}{subsection.8.6.3}}
\newlabel{treeshap}{{8.6.3}{309}{TreeSHAP}{subsection.8.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.4}Examples}{310}{subsection.8.6.4}}
\newlabel{examples-5}{{8.6.4}{310}{Examples}{subsection.8.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.5}SHAP Feature Importance}{311}{subsection.8.6.5}}
\newlabel{shap-feature-importance}{{8.6.5}{311}{SHAP Feature Importance}{subsection.8.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.24}{\ignorespaces SHAP values to explain the predicted cancer probabilities of two individuals. The baseline -- the average predicted probability -- is 0.066. The first woman has a low predicted risk of 0.06. Risk increasing effects such as STDs are offset by decreasing effects such as age. The second woman has a high predicted risk of 0.71. Age of 51 and 34 years of smoking increase her predicted cancer risk.}}{312}{figure.8.24}}
\newlabel{fig:unnamed-chunk-44}{{8.24}{312}{SHAP values to explain the predicted cancer probabilities of two individuals. The baseline -- the average predicted probability -- is 0.066. The first woman has a low predicted risk of 0.06. Risk increasing effects such as STDs are offset by decreasing effects such as age. The second woman has a high predicted risk of 0.71. Age of 51 and 34 years of smoking increase her predicted cancer risk}{figure.8.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.25}{\ignorespaces SHAP feature importance measured as the mean absolute Shapley values. The number of years with hormonal contraceptives was the most important feature, changing the predicted absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis).}}{313}{figure.8.25}}
\newlabel{fig:unnamed-chunk-45}{{8.25}{313}{SHAP feature importance measured as the mean absolute Shapley values. The number of years with hormonal contraceptives was the most important feature, changing the predicted absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis)}{figure.8.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.6}SHAP Summary Plot}{313}{subsection.8.6.6}}
\newlabel{shap-summary-plot}{{8.6.6}{313}{SHAP Summary Plot}{subsection.8.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.26}{\ignorespaces SHAP summary plot. Low number of years on hormonal contraceptives reduce the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model and are not necessarily causal in the real world.}}{314}{figure.8.26}}
\newlabel{fig:unnamed-chunk-46}{{8.26}{314}{SHAP summary plot. Low number of years on hormonal contraceptives reduce the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model and are not necessarily causal in the real world}{figure.8.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.7}SHAP Dependence Plot}{314}{subsection.8.6.7}}
\newlabel{shap-dependence-plot}{{8.6.7}{314}{SHAP Dependence Plot}{subsection.8.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.27}{\ignorespaces SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increases the predicted cancer probability.}}{315}{figure.8.27}}
\newlabel{fig:unnamed-chunk-47}{{8.27}{315}{SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increases the predicted cancer probability}{figure.8.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.8}SHAP Interaction Values}{315}{subsection.8.6.8}}
\newlabel{shap-interaction-values}{{8.6.8}{315}{SHAP Interaction Values}{subsection.8.6.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.9}Clustering SHAP values}{316}{subsection.8.6.9}}
\newlabel{clustering-shap-values}{{8.6.9}{316}{Clustering SHAP values}{subsection.8.6.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.28}{\ignorespaces SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases close to 0 years, the occurence of an STD increases the predicted cancer risk. For more years on contraceptives, the occurence of an STD reduces the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits).}}{317}{figure.8.28}}
\newlabel{fig:unnamed-chunk-48}{{8.28}{317}{SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases close to 0 years, the occurence of an STD increases the predicted cancer risk. For more years on contraceptives, the occurence of an STD reduces the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits)}{figure.8.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.10}Advantages}{317}{subsection.8.6.10}}
\newlabel{advantages-17}{{8.6.10}{317}{Advantages}{subsection.8.6.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.29}{\ignorespaces Stacked SHAP explanations clustered by explanation similarity. Each position on the x-axis is an instance of the data. Red SHAP values increase the prediction, blue values decrease it. A cluster stands out: On the right is a group with a high predicted cancer risk.}}{318}{figure.8.29}}
\newlabel{fig:unnamed-chunk-49}{{8.29}{318}{Stacked SHAP explanations clustered by explanation similarity. Each position on the x-axis is an instance of the data. Red SHAP values increase the prediction, blue values decrease it. A cluster stands out: On the right is a group with a high predicted cancer risk}{figure.8.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.11}Disadvantages}{318}{subsection.8.6.11}}
\newlabel{disadvantages-17}{{8.6.11}{318}{Disadvantages}{subsection.8.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.12}Software}{319}{subsection.8.6.12}}
\newlabel{software-4}{{8.6.12}{319}{Software}{subsection.8.6.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Neural Network Interpretation}{321}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{neural-networks}{{9}{321}{Neural Network Interpretation}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Learned Features}{323}{section.9.1}}
\newlabel{cnn-features}{{9.1}{323}{Learned Features}{section.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/.}}{323}{figure.9.1}}
\newlabel{fig:unnamed-chunk-53}{{9.1}{323}{Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/}{figure.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Feature Visualization}{324}{subsection.9.1.1}}
\newlabel{feature-visualization}{{9.1.1}{324}{Feature Visualization}{subsection.9.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1.1}Feature Visualization through Optimization}{324}{subsubsection.9.1.1.1}}
\newlabel{feature-visualization-through-optimization}{{9.1.1.1}{324}{Feature Visualization through Optimization}{subsubsection.9.1.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Feature visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron)}}{325}{figure.9.2}}
\newlabel{fig:units}{{9.2}{325}{Feature visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron)}{figure.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something which seems to have eyes yields a negative activation. [Code available in colab notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative\_neurons.ipynb)}}{326}{figure.9.3}}
\newlabel{fig:pos-neg}{{9.3}{326}{Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something which seems to have eyes yields a negative activation. [Code available in colab notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative\_neurons.ipynb)}{figure.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0), [distill.pub](https://distill.pub/2017/feature-visualization/).}}{327}{figure.9.4}}
\newlabel{fig:activation-optim}{{9.4}{327}{Iterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0), [distill.pub](https://distill.pub/2017/feature-visualization/)}{figure.9.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1.2}Connection to Adversarial Examples}{327}{subsubsection.9.1.1.2}}
\newlabel{connection-to-adversarial-examples}{{9.1.1.2}{327}{Connection to Adversarial Examples}{subsubsection.9.1.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1.3}Text and Tabular Data}{328}{subsubsection.9.1.1.3}}
\newlabel{text-and-tabular-data}{{9.1.1.3}{328}{Text and Tabular Data}{subsubsection.9.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}Network Dissection}{329}{subsection.9.1.2}}
\newlabel{network-dissection}{{9.1.2}{329}{Network Dissection}{subsection.9.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2.1}Network Dissection Algorithm}{330}{subsubsection.9.1.2.1}}
\newlabel{network-dissection-algorithm}{{9.1.2.1}{330}{Network Dissection Algorithm}{subsubsection.9.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces For a given input image and a trained network (fixed weights), we forward propagate the image up to the target layer, upscale the activations to match the original image size and compare the maximum activations with the ground truth pixel-wise segmentation. Figure originally from Bau \& Zhou et. al (2017).}}{330}{figure.9.5}}
\newlabel{fig:unnamed-chunk-54}{{9.5}{330}{For a given input image and a trained network (fixed weights), we forward propagate the image up to the target layer, upscale the activations to match the original image size and compare the maximum activations with the ground truth pixel-wise segmentation. Figure originally from Bau \& Zhou et. al (2017)}{figure.9.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Example images from the Broden dataset. Figure originally from Bau \& Zhou et. al (2017).}}{331}{figure.9.6}}
\newlabel{fig:unnamed-chunk-55}{{9.6}{331}{Example images from the Broden dataset. Figure originally from Bau \& Zhou et. al (2017)}{figure.9.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2.2}Experiments}{332}{subsubsection.9.1.2.2}}
\newlabel{experiments}{{9.1.2.2}{332}{Experiments}{subsubsection.9.1.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces The Intersection over Union (IoU) is computed by comparing the human ground truth annotation and the top activated pixels.}}{333}{figure.9.7}}
\newlabel{fig:unnamed-chunk-56}{{9.7}{333}{The Intersection over Union (IoU) is computed by comparing the human ground truth annotation and the top activated pixels}{figure.9.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Activation mask for inception\_4e channel 750 which detects dogs with $IoU=0.203$. Figure originally from Bau \& Zhou et. al (2017).}}{333}{figure.9.8}}
\newlabel{fig:unnamed-chunk-57}{{9.8}{333}{Activation mask for inception\_4e channel 750 which detects dogs with $IoU=0.203$. Figure originally from Bau \& Zhou et. al (2017)}{figure.9.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces ResNet trained on Places365 has the highest number of unique detectors. AlexNet with random weights has the lowest number of unique detectors and serves as baseline. Figure originally from Bau \& Zhou et. al (2017).}}{334}{figure.9.9}}
\newlabel{fig:unnamed-chunk-58}{{9.9}{334}{ResNet trained on Places365 has the highest number of unique detectors. AlexNet with random weights has the lowest number of unique detectors and serves as baseline. Figure originally from Bau \& Zhou et. al (2017)}{figure.9.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Advantages}{335}{subsection.9.1.3}}
\newlabel{advantages-18}{{9.1.3}{335}{Advantages}{subsection.9.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces The number of unique concept detectors decreases when the 256 channels of AlexNet conv5 (trained on ImageNet) are gradually changed to a basis using a random orthogonal transformation. Figure originally from Bau \& Zhou et. al (2017).}}{336}{figure.9.10}}
\newlabel{fig:unnamed-chunk-59}{{9.10}{336}{The number of unique concept detectors decreases when the 256 channels of AlexNet conv5 (trained on ImageNet) are gradually changed to a basis using a random orthogonal transformation. Figure originally from Bau \& Zhou et. al (2017)}{figure.9.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}Disadvantages}{337}{subsection.9.1.4}}
\newlabel{disadvantages-18}{{9.1.4}{337}{Disadvantages}{subsection.9.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.5}Software and Further Material}{338}{subsection.9.1.5}}
\newlabel{software-and-further-material}{{9.1.5}{338}{Software and Further Material}{subsection.9.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Pixel Attribution (Saliency Maps)}{339}{section.9.2}}
\newlabel{pixel-attribution}{{9.2}{339}{Pixel Attribution (Saliency Maps)}{section.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces A saliency map in which pixels are colored by their contribution to the classification.}}{339}{figure.9.11}}
\newlabel{fig:unnamed-chunk-61}{{9.11}{339}{A saliency map in which pixels are colored by their contribution to the classification}{figure.9.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Vanilla Gradient (Saliency Maps)}{341}{subsection.9.2.1}}
\newlabel{vanilla-gradient-saliency-maps}{{9.2.1}{341}{Vanilla Gradient (Saliency Maps)}{subsection.9.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1.1}Problems with Vanilla Gradient}{343}{subsubsection.9.2.1.1}}
\newlabel{problems-with-vanilla-gradient}{{9.2.1.1}{343}{Problems with Vanilla Gradient}{subsubsection.9.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}DeconvNet}{344}{subsection.9.2.2}}
\newlabel{deconvnet}{{9.2.2}{344}{DeconvNet}{subsection.9.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Grad-CAM}{344}{subsection.9.2.3}}
\newlabel{grad-cam}{{9.2.3}{344}{Grad-CAM}{subsection.9.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Guided Grad-CAM}{347}{subsection.9.2.4}}
\newlabel{guided-grad-cam}{{9.2.4}{347}{Guided Grad-CAM}{subsection.9.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.5}SmoothGrad}{347}{subsection.9.2.5}}
\newlabel{smoothgrad}{{9.2.5}{347}{SmoothGrad}{subsection.9.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.6}Examples}{348}{subsection.9.2.6}}
\newlabel{examples-6}{{9.2.6}{348}{Examples}{subsection.9.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces Images of a dog classified as greyhound, a ramen soup classified as soup bowl, and an octopus classified as eel.}}{348}{figure.9.12}}
\newlabel{fig:unnamed-chunk-62}{{9.12}{348}{Images of a dog classified as greyhound, a ramen soup classified as soup bowl, and an octopus classified as eel}{figure.9.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces Pixel attributions or saliency maps for the vanilla method, smoothgrad and Grad-Cam.}}{349}{figure.9.13}}
\newlabel{fig:unnamed-chunk-63}{{9.13}{349}{Pixel attributions or saliency maps for the vanilla method, smoothgrad and Grad-Cam}{figure.9.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.7}Advantages}{350}{subsection.9.2.7}}
\newlabel{advantages-19}{{9.2.7}{350}{Advantages}{subsection.9.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.8}Disadvantages}{351}{subsection.9.2.8}}
\newlabel{disadvantages-19}{{9.2.8}{351}{Disadvantages}{subsection.9.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.9}Software}{352}{subsection.9.2.9}}
\newlabel{software-5}{{9.2.9}{352}{Software}{subsection.9.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Detecting Concepts}{353}{section.9.3}}
\newlabel{detecting-concepts}{{9.3}{353}{Detecting Concepts}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}TCAV: Testing with Concept Activation Vectors}{353}{subsection.9.3.1}}
\newlabel{tcav-testing-with-concept-activation-vectors}{{9.3.1}{353}{TCAV: Testing with Concept Activation Vectors}{subsection.9.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.1.1}Concept Activation Vector (CAV)}{354}{subsubsection.9.3.1.1}}
\newlabel{concept-activation-vector-cav}{{9.3.1.1}{354}{Concept Activation Vector (CAV)}{subsubsection.9.3.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.1.2}Testing with CAVs (TCAV)}{355}{subsubsection.9.3.1.2}}
\newlabel{testing-with-cavs-tcav}{{9.3.1.2}{355}{Testing with CAVs (TCAV)}{subsubsection.9.3.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Example}{356}{subsection.9.3.2}}
\newlabel{example-9}{{9.3.2}{356}{Example}{subsection.9.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Advantages}{356}{subsection.9.3.3}}
\newlabel{advantages-20}{{9.3.3}{356}{Advantages}{subsection.9.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.14}{\ignorespaces The example of measuring TCAV scores of three concepts for the model predicting “zebra”. The targeted bottleneck is a layer called “mixed4c”. A star sign above “dotted” indicates that “dotted” has not passed the statistical significance test, i.e., having the p-value larger than 0.05. Both “striped” and “zigzagged” have passed the test, and both concepts are useful for the model to identify “zebra” images according to TCAV. Figure originally from the TCAV GitHub.}}{357}{figure.9.14}}
\newlabel{fig:unnamed-chunk-64}{{9.14}{357}{The example of measuring TCAV scores of three concepts for the model predicting “zebra”. The targeted bottleneck is a layer called “mixed4c”. A star sign above “dotted” indicates that “dotted” has not passed the statistical significance test, i.e., having the p-value larger than 0.05. Both “striped” and “zigzagged” have passed the test, and both concepts are useful for the model to identify “zebra” images according to TCAV. Figure originally from the TCAV GitHub}{figure.9.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Disadvantages}{358}{subsection.9.3.4}}
\newlabel{disadvantages-20}{{9.3.4}{358}{Disadvantages}{subsection.9.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Bonus: Other Concept-based Approaches}{358}{subsection.9.3.5}}
\newlabel{bonus-other-concept-based-approaches}{{9.3.5}{358}{Bonus: Other Concept-based Approaches}{subsection.9.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.6}Software}{359}{subsection.9.3.6}}
\newlabel{software-6}{{9.3.6}{359}{Software}{subsection.9.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Adversarial Examples}{360}{section.9.4}}
\newlabel{adversarial}{{9.4}{360}{Adversarial Examples}{section.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Methods and Examples}{360}{subsection.9.4.1}}
\newlabel{methods-and-examples}{{9.4.1}{360}{Methods and Examples}{subsection.9.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.15}{\ignorespaces Adversarial examples for AlexNet by Szegedy et. al (2013). All images in the left column are correctly classified. The middle column shows the (magnified) error added to the images to produce the images in the right column all categorized (incorrectly) as "Ostrich". "Intriguing properties of neural networks", Figure 5 by Szegedy et. al. CC-BY 3.0.}}{362}{figure.9.15}}
\newlabel{fig:adversarial-ostrich}{{9.15}{362}{Adversarial examples for AlexNet by Szegedy et. al (2013). All images in the left column are correctly classified. The middle column shows the (magnified) error added to the images to produce the images in the right column all categorized (incorrectly) as "Ostrich". "Intriguing properties of neural networks", Figure 5 by Szegedy et. al. CC-BY 3.0}{figure.9.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.16}{\ignorespaces By intentionally changing a single pixel a neural network trained on ImageNet can be deceived to predict the wrong class instead of the original class.}}{364}{figure.9.16}}
\newlabel{fig:adversarial-1pixel}{{9.16}{364}{By intentionally changing a single pixel a neural network trained on ImageNet can be deceived to predict the wrong class instead of the original class}{figure.9.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.17}{\ignorespaces A sticker that makes a VGG16 classifier trained on ImageNet categorize an image of a banana as a toaster. Work by Brown et. al (2017).}}{365}{figure.9.17}}
\newlabel{fig:adversarial-toaster}{{9.17}{365}{A sticker that makes a VGG16 classifier trained on ImageNet categorize an image of a banana as a toaster. Work by Brown et. al (2017)}{figure.9.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.18}{\ignorespaces Athalye et. al (2017) created a 3D-printed that is recognized as a rifle by TensorFlow’s standard pre-trained InceptionV3 classifier.}}{366}{figure.9.18}}
\newlabel{fig:adversarial-turtle}{{9.18}{366}{Athalye et. al (2017) created a 3D-printed that is recognized as a rifle by TensorFlow’s standard pre-trained InceptionV3 classifier}{figure.9.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}The Cybersecurity Perspective}{369}{subsection.9.4.2}}
\newlabel{the-cybersecurity-perspective}{{9.4.2}{369}{The Cybersecurity Perspective}{subsection.9.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Influential Instances}{372}{section.9.5}}
\newlabel{influential}{{9.5}{372}{Influential Instances}{section.9.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.19}{\ignorespaces Feature x follows a Gaussian distribution with an outlier at x=8.}}{374}{figure.9.19}}
\newlabel{fig:outlier}{{9.19}{374}{Feature x follows a Gaussian distribution with an outlier at x=8}{figure.9.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.20}{\ignorespaces A linear model with one feature. Trained once on the full data and once without the influential instance. Removing the influential instance changes the fitted slope (weight/coefficient) drastically.}}{375}{figure.9.20}}
\newlabel{fig:influential-point}{{9.20}{375}{A linear model with one feature. Trained once on the full data and once without the influential instance. Removing the influential instance changes the fitted slope (weight/coefficient) drastically}{figure.9.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.21}{\ignorespaces A learner learns a model from training data (features plus target). The model makes predictions for new data.}}{375}{figure.9.21}}
\newlabel{fig:learner}{{9.21}{375}{A learner learns a model from training data (features plus target). The model makes predictions for new data}{figure.9.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Deletion Diagnostics}{376}{subsection.9.5.1}}
\newlabel{deletion-diagnostics}{{9.5.1}{376}{Deletion Diagnostics}{subsection.9.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.22}{\ignorespaces A decision tree that models the relationship between the influence of the instances and their features. The maximum depth of the tree is set to 2.}}{380}{figure.9.22}}
\newlabel{fig:cooks-analyzed-include}{{9.22}{380}{A decision tree that models the relationship between the influence of the instances and their features. The maximum depth of the tree is set to 2}{figure.9.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.23}{\ignorespaces Decision tree that explains which instances were most influential for predicting the 7-th instance. Data from women who smoked for 18.5 years or longer had a large influence on the prediction of the 7-th instance, with an average change in absolute prediction by 11.7 percentage points of cancer probability.}}{381}{figure.9.23}}
\newlabel{fig:influence-single-include}{{9.23}{381}{Decision tree that explains which instances were most influential for predicting the 7-th instance. Data from women who smoked for 18.5 years or longer had a large influence on the prediction of the 7-th instance, with an average change in absolute prediction by 11.7 percentage points of cancer probability}{figure.9.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Influence Functions}{382}{subsection.9.5.2}}
\newlabel{influence-functions}{{9.5.2}{382}{Influence Functions}{subsection.9.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.24}{\ignorespaces Updating the model parameter (x-axis) by forming a quadratic expansion of the loss around the current model parameter, and moving 1/n into the direction in which the loss with upweighted instance z (y-axis) improves most. This upweighting of instance z in the loss approximates the parameter changes if we delete z and train the model on the reduced data.}}{386}{figure.9.24}}
\newlabel{fig:quadratic-expansion}{{9.24}{386}{Updating the model parameter (x-axis) by forming a quadratic expansion of the loss around the current model parameter, and moving 1/n into the direction in which the loss with upweighted instance z (y-axis) improves most. This upweighting of instance z in the loss approximates the parameter changes if we delete z and train the model on the reduced data}{figure.9.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.3}Advantages of Identifying Influential Instances}{388}{subsection.9.5.3}}
\newlabel{advantages-of-identifying-influential-instances}{{9.5.3}{388}{Advantages of Identifying Influential Instances}{subsection.9.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.4}Disadvantages of Identifying Influential Instances}{389}{subsection.9.5.4}}
\newlabel{disadvantages-of-identifying-influential-instances}{{9.5.4}{389}{Disadvantages of Identifying Influential Instances}{subsection.9.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.5}Software and Alternatives}{391}{subsection.9.5.5}}
\newlabel{software-and-alternatives-6}{{9.5.5}{391}{Software and Alternatives}{subsection.9.5.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}A Look into the Crystal Ball}{393}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{future}{{10}{393}{A Look into the Crystal Ball}{chapter.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}The Future of Machine Learning}{395}{section.10.1}}
\newlabel{the-future-of-machine-learning}{{10.1}{395}{The Future of Machine Learning}{section.10.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}The Future of Interpretability}{397}{section.10.2}}
\newlabel{the-future-of-interpretability}{{10.2}{397}{The Future of Interpretability}{section.10.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Contribute to the Book}{401}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{contribute}{{11}{401}{Contribute to the Book}{chapter.11}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Citing this Book}{403}{chapter.12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cite}{{12}{403}{Citing this Book}{chapter.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Translations}{405}{chapter.13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{translations}{{13}{405}{Translations}{chapter.13}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Acknowledgements}{407}{chapter.14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{acknowledgements}{{14}{407}{Acknowledgements}{chapter.14}{}}
\@writefile{toc}{\contentsline {fm}{References}{409}{chapter*.6}}
\newlabel{references}{{14}{409}{References}{chapter*.6}{}}
\newlabel{r-packages-used}{{14}{417}{R Packages Used}{section*.7}{}}
