<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.2 Feature Attribution for Image Classification | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="7.2 Feature Attribution for Image Classification | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.2 Feature Attribution for Image Classification | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2021-01-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="cnn-features.html"/>
<link rel="next" href="future.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> Story Time</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi's Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> Importance of Interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="taxonomy-of-interpretability-methods.html"><a href="taxonomy-of-interpretability-methods.html"><i class="fa fa-check"></i><b>2.2</b> Taxonomy of Interpretability Methods</a></li>
<li class="chapter" data-level="2.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>2.3</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.3.1</b> Algorithm Transparency</a></li>
<li class="chapter" data-level="2.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>2.3.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="2.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>2.3.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="2.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>2.3.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="2.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>2.3.5</b> Local Interpretability for a Group of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="evaluation-of-interpretability.html"><a href="evaluation-of-interpretability.html"><i class="fa fa-check"></i><b>2.4</b> Evaluation of Interpretability</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> Properties of Explanations</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> Human-friendly Explanations</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>2.6.1</b> What Is an Explanation?</a></li>
<li class="chapter" data-level="2.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>2.6.2</b> What Is a Good Explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike Rentals (Regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Interpretable Models</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>4.1.2</b> Example</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>4.1.3</b> Visual Interpretation</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>4.1.4</b> Explain Individual Predictions</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>4.1.5</b> Encoding of Categorical Features</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>4.1.6</b> Do Linear Models Create Good Explanations?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> Sparse Linear Models</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>4.1.8</b> Advantages</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>4.1.9</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#what-is-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>4.2.1</b> What is Wrong with Linear Regression for Classification?</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#theory"><i class="fa fa-check"></i><b>4.2.2</b> Theory</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>4.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#example-1"><i class="fa fa-check"></i><b>4.2.4</b> Example</a></li>
<li class="chapter" data-level="4.2.5" data-path="logistic.html"><a href="logistic.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>4.2.5</b> Advantages and Disadvantages</a></li>
<li class="chapter" data-level="4.2.6" data-path="logistic.html"><a href="logistic.html#software"><i class="fa fa-check"></i><b>4.2.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.3</b> GLM, GAM and more</a><ul>
<li class="chapter" data-level="4.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.3.1</b> Non-Gaussian Outcomes - GLMs</a></li>
<li class="chapter" data-level="4.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.3.2</b> Interactions</a></li>
<li class="chapter" data-level="4.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.3.3</b> Nonlinear Effects - GAMs</a></li>
<li class="chapter" data-level="4.3.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>4.3.4</b> Advantages</a></li>
<li class="chapter" data-level="4.3.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>4.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.3.6" data-path="extend-lm.html"><a href="extend-lm.html#software-1"><i class="fa fa-check"></i><b>4.3.6</b> Software</a></li>
<li class="chapter" data-level="4.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.3.7</b> Further Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> Decision Tree</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>4.4.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#example-2"><i class="fa fa-check"></i><b>4.4.2</b> Example</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#advantages-2"><i class="fa fa-check"></i><b>4.4.3</b> Advantages</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#disadvantages-2"><i class="fa fa-check"></i><b>4.4.4</b> Disadvantages</a></li>
<li class="chapter" data-level="4.4.5" data-path="tree.html"><a href="tree.html#software-2"><i class="fa fa-check"></i><b>4.4.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.5</b> Decision Rules</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>4.5.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="4.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.5.4" data-path="rules.html"><a href="rules.html#advantages-3"><i class="fa fa-check"></i><b>4.5.4</b> Advantages</a></li>
<li class="chapter" data-level="4.5.5" data-path="rules.html"><a href="rules.html#disadvantages-3"><i class="fa fa-check"></i><b>4.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.5.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>4.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.6</b> RuleFit</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>4.6.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="4.6.2" data-path="rulefit.html"><a href="rulefit.html#theory-1"><i class="fa fa-check"></i><b>4.6.2</b> Theory</a></li>
<li class="chapter" data-level="4.6.3" data-path="rulefit.html"><a href="rulefit.html#advantages-4"><i class="fa fa-check"></i><b>4.6.3</b> Advantages</a></li>
<li class="chapter" data-level="4.6.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-4"><i class="fa fa-check"></i><b>4.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="4.6.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>4.6.5</b> Software and Alternative</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.7</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="4.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>4.7.1</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="4.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>4.7.2</b> K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>5.1.1</b> Examples</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#advantages-5"><i class="fa fa-check"></i><b>5.1.2</b> Advantages</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#disadvantages-5"><i class="fa fa-check"></i><b>5.1.3</b> Disadvantages</a></li>
<li class="chapter" data-level="5.1.4" data-path="pdp.html"><a href="pdp.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>5.1.4</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#examples-1"><i class="fa fa-check"></i><b>5.2.1</b> Examples</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#advantages-6"><i class="fa fa-check"></i><b>5.2.2</b> Advantages</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#disadvantages-6"><i class="fa fa-check"></i><b>5.2.3</b> Disadvantages</a></li>
<li class="chapter" data-level="5.2.4" data-path="ice.html"><a href="ice.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>5.2.4</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>5.3.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="5.3.2" data-path="ale.html"><a href="ale.html#theory-2"><i class="fa fa-check"></i><b>5.3.2</b> Theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>5.3.3</b> Estimation</a></li>
<li class="chapter" data-level="5.3.4" data-path="ale.html"><a href="ale.html#examples-2"><i class="fa fa-check"></i><b>5.3.4</b> Examples</a></li>
<li class="chapter" data-level="5.3.5" data-path="ale.html"><a href="ale.html#advantages-7"><i class="fa fa-check"></i><b>5.3.5</b> Advantages</a></li>
<li class="chapter" data-level="5.3.6" data-path="ale.html"><a href="ale.html#disadvantages-7"><i class="fa fa-check"></i><b>5.3.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.3.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>5.3.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.4</b> Feature Interaction</a><ul>
<li class="chapter" data-level="5.4.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>5.4.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="5.4.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>5.4.2</b> Theory: Friedman's H-statistic</a></li>
<li class="chapter" data-level="5.4.3" data-path="interaction.html"><a href="interaction.html#examples-3"><i class="fa fa-check"></i><b>5.4.3</b> Examples</a></li>
<li class="chapter" data-level="5.4.4" data-path="interaction.html"><a href="interaction.html#advantages-8"><i class="fa fa-check"></i><b>5.4.4</b> Advantages</a></li>
<li class="chapter" data-level="5.4.5" data-path="interaction.html"><a href="interaction.html#disadvantages-8"><i class="fa fa-check"></i><b>5.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.4.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>5.4.6</b> Implementations</a></li>
<li class="chapter" data-level="5.4.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>5.4.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.5.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-3"><i class="fa fa-check"></i><b>5.5.1</b> Theory</a></li>
<li class="chapter" data-level="5.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.5.2</b> Should I Compute Importance on Training or Test Data?</a></li>
<li class="chapter" data-level="5.5.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>5.5.3</b> Example and Interpretation</a></li>
<li class="chapter" data-level="5.5.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-9"><i class="fa fa-check"></i><b>5.5.4</b> Advantages</a></li>
<li class="chapter" data-level="5.5.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-9"><i class="fa fa-check"></i><b>5.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.6" data-path="feature-importance.html"><a href="feature-importance.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>5.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.6</b> Global Surrogate</a><ul>
<li class="chapter" data-level="5.6.1" data-path="global.html"><a href="global.html#theory-4"><i class="fa fa-check"></i><b>5.6.1</b> Theory</a></li>
<li class="chapter" data-level="5.6.2" data-path="global.html"><a href="global.html#example-4"><i class="fa fa-check"></i><b>5.6.2</b> Example</a></li>
<li class="chapter" data-level="5.6.3" data-path="global.html"><a href="global.html#advantages-10"><i class="fa fa-check"></i><b>5.6.3</b> Advantages</a></li>
<li class="chapter" data-level="5.6.4" data-path="global.html"><a href="global.html#disadvantages-10"><i class="fa fa-check"></i><b>5.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.6.5" data-path="global.html"><a href="global.html#software-3"><i class="fa fa-check"></i><b>5.6.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.7</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>5.7.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="5.7.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>5.7.2</b> LIME for Text</a></li>
<li class="chapter" data-level="5.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.7.3</b> LIME for Images</a></li>
<li class="chapter" data-level="5.7.4" data-path="lime.html"><a href="lime.html#advantages-11"><i class="fa fa-check"></i><b>5.7.4</b> Advantages</a></li>
<li class="chapter" data-level="5.7.5" data-path="lime.html"><a href="lime.html#disadvantages-11"><i class="fa fa-check"></i><b>5.7.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.8</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>5.8.1</b> Finding Anchors</a></li>
<li class="chapter" data-level="5.8.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>5.8.2</b> Complexity and Runtime</a></li>
<li class="chapter" data-level="5.8.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>5.8.3</b> Tabular Data Example</a></li>
<li class="chapter" data-level="5.8.4" data-path="anchors.html"><a href="anchors.html#advantages-12"><i class="fa fa-check"></i><b>5.8.4</b> Advantages</a></li>
<li class="chapter" data-level="5.8.5" data-path="anchors.html"><a href="anchors.html#disadvantages-12"><i class="fa fa-check"></i><b>5.8.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.8.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>5.8.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.9</b> Shapley Values</a><ul>
<li class="chapter" data-level="5.9.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>5.9.1</b> General Idea</a></li>
<li class="chapter" data-level="5.9.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>5.9.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="5.9.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>5.9.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="5.9.4" data-path="shapley.html"><a href="shapley.html#advantages-13"><i class="fa fa-check"></i><b>5.9.4</b> Advantages</a></li>
<li class="chapter" data-level="5.9.5" data-path="shapley.html"><a href="shapley.html#disadvantages-13"><i class="fa fa-check"></i><b>5.9.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.9.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-5"><i class="fa fa-check"></i><b>5.9.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.10</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>5.10.1</b> Definition</a></li>
<li class="chapter" data-level="5.10.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.10.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.10.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.10.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.10.4" data-path="shap.html"><a href="shap.html#examples-4"><i class="fa fa-check"></i><b>5.10.4</b> Examples</a></li>
<li class="chapter" data-level="5.10.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>5.10.5</b> SHAP Feature Importance</a></li>
<li class="chapter" data-level="5.10.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.10.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.10.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.10.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.10.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>5.10.8</b> SHAP Interaction Values</a></li>
<li class="chapter" data-level="5.10.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.10.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.10.10" data-path="shap.html"><a href="shap.html#advantages-14"><i class="fa fa-check"></i><b>5.10.10</b> Advantages</a></li>
<li class="chapter" data-level="5.10.11" data-path="shap.html"><a href="shap.html#disadvantages-14"><i class="fa fa-check"></i><b>5.10.11</b> Disadvantages</a></li>
<li class="chapter" data-level="5.10.12" data-path="shap.html"><a href="shap.html#software-4"><i class="fa fa-check"></i><b>5.10.12</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> Example-Based Explanations</a><ul>
<li class="chapter" data-level="6.1" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>6.1</b> Counterfactual Explanations</a><ul>
<li class="chapter" data-level="6.1.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Counterfactual Explanations</a></li>
<li class="chapter" data-level="6.1.2" data-path="counterfactual.html"><a href="counterfactual.html#examples-5"><i class="fa fa-check"></i><b>6.1.2</b> Examples</a></li>
<li class="chapter" data-level="6.1.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-15"><i class="fa fa-check"></i><b>6.1.3</b> Advantages</a></li>
<li class="chapter" data-level="6.1.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-15"><i class="fa fa-check"></i><b>6.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.1.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>6.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>6.2.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>6.2.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#theory-5"><i class="fa fa-check"></i><b>6.3.1</b> Theory</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#examples-6"><i class="fa fa-check"></i><b>6.3.2</b> Examples</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#advantages-16"><i class="fa fa-check"></i><b>6.3.3</b> Advantages</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#disadvantages-16"><i class="fa fa-check"></i><b>6.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>6.3.5</b> Code and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> Influence Functions</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-6"><i class="fa fa-check"></i><b>6.4.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Network Interpretation</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> Learned Features</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>7.1.1</b> Feature Visualization</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>7.1.2</b> Network Dissection</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-17"><i class="fa fa-check"></i><b>7.1.3</b> Advantages</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-17"><i class="fa fa-check"></i><b>7.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>7.1.5</b> Software and Further Material</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html"><i class="fa fa-check"></i><b>7.2</b> Feature Attribution for Image Classification</a><ul>
<li class="chapter" data-level="7.2.1" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#vanilla-gradient-saliency-maps"><i class="fa fa-check"></i><b>7.2.1</b> Vanilla Gradient (Saliency Maps)</a></li>
<li class="chapter" data-level="7.2.2" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#deconvnet"><i class="fa fa-check"></i><b>7.2.2</b> DeconvNet</a></li>
<li class="chapter" data-level="7.2.3" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#grad-cam"><i class="fa fa-check"></i><b>7.2.3</b> Grad-CAM</a></li>
<li class="chapter" data-level="7.2.4" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#guided-gradcam"><i class="fa fa-check"></i><b>7.2.4</b> Guided GradCAM</a></li>
<li class="chapter" data-level="7.2.5" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#smoothgrad"><i class="fa fa-check"></i><b>7.2.5</b> SmoothGrad</a></li>
<li class="chapter" data-level="7.2.6" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#alternative-perturbation-based-methods"><i class="fa fa-check"></i><b>7.2.6</b> Alternative: Perturbation-based Methods</a></li>
<li class="chapter" data-level="7.2.7" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#examples-7"><i class="fa fa-check"></i><b>7.2.7</b> Examples</a></li>
<li class="chapter" data-level="7.2.8" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#advantages-18"><i class="fa fa-check"></i><b>7.2.8</b> Advantages</a></li>
<li class="chapter" data-level="7.2.9" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#disadvantages-18"><i class="fa fa-check"></i><b>7.2.9</b> Disadvantages</a></li>
<li class="chapter" data-level="7.2.10" data-path="feature-attribution-for-image-classification.html"><a href="feature-attribution-for-image-classification.html#software-5"><i class="fa fa-check"></i><b>7.2.10</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="8.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>8.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="8.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>8.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> Contribute to the Book</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> Citing this Book</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> Translations</a></li>
<li class="chapter" data-level="12" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>12</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used-for-examples.html"><a href="r-packages-used-for-examples.html"><i class="fa fa-check"></i>R Packages Used for Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="feature-attribution-for-image-classification" class="section level2">
<h2><span class="header-section-number">7.2</span> Feature Attribution for Image Classification</h2>
<p><em>This chapter is currently only available in this web version. ebook and print will follow.</em></p>
<!--
Some literature
- For feature attribution: http://blog.qure.ai/notes/deep-learning-visualization-gradient-based-methods
- http://blog.qure.ai/notes/visualizing_deep_learning
- Also check out: https://openreview.net/pdf?id=Hkn7CBaTW
- https://www.youtube.com/watch?v=h9Y3FI_3lBw
- Good overview: TOWARDS BETTER UNDERSTANDING OF GRADIENT - BASED ATTRIBUTION METHODS FOR DEEP NEURAL NETWORKS [^better-understanding], 2018
-->
<!--Idea for chapter:
- Start with general goal
- Intuition for how methods work
- Categorization of methods? [maybe later in chapter, after 3 approaches]
- Roughly explain 3 different approaches:
    - Vanilla Gradient: Because simple and educational.
    - Grad-CAM (or SmoothGRAD?) because it works
    - Some other which is not GradCAM and still works. Integrated Gradients? LRP?
- Application [VGG16, INNvestigate]
- Problems of attribution methods
- List of methods that are excluded because they don't work
- List of more methods
- Advantages / Disadvantages
- Software
-->
<!--
Questions
- How do those methods handle the RGB dimensionality? Average?
-->
<!-- short summary -->
<p>Saliency maps are &quot;heatmaps&quot; that are often overlayed on images to highlight the pixels that were responsible for a certain neural network classification. They are based on backpropagation of the gradient. And they look a bit like this:</p>
<p><img src="images/vanilla.png" width="80%" /></p>
<p>You will see later in the chapter what is going on in that particular image. Other names that are used in this context: sensitivity map, saliency map, pixel attriution map, gradient-based attribution methods, feature relevance, feature attribution, feature contribution</p>
<!-- General idea and distinction -->
<p>Saliency maps are feature attribution methods for images that attribute the prediction to the input features. Feature attribution explains individual predictions by attributing each input feature by how much it changed the prediction (negative or positive). Be it input pixels, tabular data or words. For saliency maps these are pixels. These methods usually do not change the network, and we will only look at methods that are post-hoc, meaning they can be applied after the network was trained as usual.</p>
<p>A bit more formal definition would be this: A feature attribution method of the prediction for p-dimensional input x, relative to some base input (or prediction?) can be expressed as a vector of relevances: <span class="math inline">\((r_1,\ldots,r_p)\)</span>. The j-th element r<sub>j</sub> is the contribution of the j-th feature input to the prediction.</p>
<p>There is a confusing amount of approaches out there to compute feature attributions. We can get a handle on them by understanding two things: 1. Some methods manipulate parts of the image (model-agnostic) and some methods compute the gradient of the prediction (or classification score) with respect to the input features. 2. The gradient based methods (of which there are many) mostly differ in how the gradient is computed.</p>
<p>All methods have in common that in the end we get an output that has the same size as the input image (or can at least be projected meaningfully onto it) and gives each pixel a value which somehow can be interpreted as the relevance of the pixel for the prediction or classification of that image.</p>
<p>There are different ways to achieve these kind of attributions:</p>
<ul>
<li>Gradient based: Look at the gradients of the input pixels. Also called saliency. The other option are perturbation based, which is model-agnostic and by replacing parts of the image with some &quot;neutral&quot; image.</li>
<li>The attribution might show only the positive contributions towards the class of interest. Or it might show, in two different colors, both positive and negative. Or it might just show localization, which are regions that are important on an image</li>
<li>Then there gradient vs. path attribution. The gradient just tells us whether a change of this pixel (increasing color) would change the prediction. The path attribution compares the current image with a reference image (which can be some artificial &quot;Null&quot; image) and distributes the difference of the prediction for both among the pixels. This article researches what is a good baseline. Link: <a href="https://arxiv.org/pdf/1703.01365.pdf" class="uri">https://arxiv.org/pdf/1703.01365.pdf</a> . Same as feature attribution, but with reference point instead. <a href="#fn76" class="footnoteRef" id="fnref76"><sup>76</sup></a> classify gradient based methods into 3 categories: Gradients, Signal Methods (DeConvNet, Guided BackProb, PatternNet) and Attribution methods (Deep Taylor, Integrated Gradients).</li>
</ul>
<p>For the pixels, attributions are averaged over the channels.</p>
<p>We only look at gradient-based methods here. Gradient-based methods compute the gradient of the output with respect to the input features. The interpretation of the gradient with respect to the input features is: If I were to change this feature, the predicted class probability would go up (for positive gradient) or down (for negative gradient). The larger the absolute value of the gradient, the stronger the effect of changes in that feature.</p>
<p>Question to myself: Using the pure gradient should not make too much sense, because it does not matter how I change the pixel, only by the absolute change in prediction?</p>
<!-- TODO: Explain the overall algorithm template here -->
<p>The general recipe for the gradient-based attribution methods is simple: All methods have one-forward pass to get the prediction and one backward pass (to get the gradients). Then we know for each pixel the gradient. But there is one obstacle, which is one reason why we have so many different approaches: Neural network have non-linear transformation units, and therefore there is some ambiguity how to exactly do the backward pass of the gradients. The difference to the normal backpropagation is that we compute the gradient with respect to the input features. Backpropagagion compute the gradient with respect to the weights of the neural network.</p>
<p>More formally, we consider neural networks that output as prediction a vector of length <span class="math inline">\(C\)</span>, which includes regression where <span class="math inline">\(C=1\)</span>. Output of DNN is called <span class="math inline">\(S(x)=[S_1(x),\ldots,S_p(x)]\)</span>. Formally all those methods take in input <span class="math inline">\(x\in\mathbb{R}^p\)</span> (can be image pixels, tabular data, words, ...) and outputs an explanation <span class="math inline">\(R^c=[R_1^c,\ldots,R_p^c]\)</span>, one relevance value for each of the p input features. The c indicicates the relevance for the c-th output.</p>
<p>In multi-class classification you have to decide for which classification to look at the relevance of the inputs. This can be the correct class of that example, at least that's an interesting case to look at. But also if DNN was wrong, it might be interesting to look at the input relevance for the wrong class and also for the relevance for the correct class for debugging.</p>
<p>The word attribution maps means that, for images, we visualize the pixels with red if they positively contributed, blue if negatively. Of course, you are free to choose any color you like.</p>
<p>Some of those methods have the property of Completeness, meaning that the sum of the relevance values for all input features sum up to the difference between the prediction of the image/data point minus the prediction of a reference point (e.g. all grey image). Integrated Gradient and SHAP have this property.</p>
<p>With all these approaches, we assume that we have an already trained CNN.</p>
<p>And we start with a rather simple approach, the &quot;vanilla&quot; gradient.</p>
<div id="vanilla-gradient-saliency-maps" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Vanilla Gradient (Saliency Maps)</h3>
<p>The idea of vanilla gradient, presented by <a href="#fn77" class="footnoteRef" id="fnref77"><sup>77</sup></a> as one of the first saliency approaches is quite simple, we compute the gradient of the loss function for the class we are interested in with respect to the input features. This gives us a map of the size of the input features with negative to positive values. Concerning naming, they called their approach &quot;Image-Specific Class Saliency&quot;.</p>
<p>The main recipe for this approach is:</p>
<ol style="list-style-type: decimal">
<li>Do a forward pass of the image of interest</li>
<li>Do a backward propagation of the desired class to the input features to get the gradient <span class="math display">\[E_{grad}(I_0)=\frac{\delta{}S_c}{\delta{}I}|_{I=I_0}\]</span> Thereby we set all other classes to zero.</li>
<li>Visualize the gradients. Either absolute or with negative / positive</li>
</ol>
<p>More formally, we have an image I and the CNN give it a score <span class="math inline">\(S_c(I)\)</span> for class c. The score is a highly non-linear function of our image. But the motivation is that we can approximate that score by applying first-order Taylor expansion</p>
<p><span class="math display">\[S_c(I)\approx{}w^T{}I+b,\]</span></p>
<p>where w is the derivate of our score:</p>
<p><span class="math display">\[w = \frac{\delta S_C}{\delta I}|_{I_0}\]</span></p>
<p>Now, there is some ambiguity how to do a backward pass of the gradients, as non-linear units such as ReLU (Rectifying Linear Unit) &quot;remove&quot; the sign. So when we do a backpass, we don't know whether to assign a positive or negative or negative activation. Using my incredible ASCII art skill, the ReLU function looks like this &quot;_/&quot; and is defined as <span class="math inline">\(X_{n+1}(x)=max(0,X_n)\)</span> from layer <span class="math inline">\(X_n\)</span> to layer <span class="math inline">\(X_{n-1}\)</span>. This means that when the activation of a neuron is 0, we do not know what to backpropagate. In the case of vanilla gradient approach, the ambiguity is resolved as:</p>
<p><span class="math inline">\($\frac{\delta f}{\delta X_n} = \frac{\delta f}{\delta X_{n+1}} \cdot \mathbb{1}(X_n &gt; 0)\)</span></p>
<p>where <span class="math inline">\(\mathbb{1}\)</span> is the element-wise indicator function which is 0 where the activation at the lower layer was negative or zero, and 1 where it is positive. Or, in words, we take the gradient that we have back-propagated so far until the layer n+1, then simply set the gradients to zero where the activation at the layer below is not positive.</p>
<p>Let's assume we have layer <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X_{n+1}=ReLU(X_{n+1})\)</span>. Our fictive activation at <span class="math inline">\(X_n\)</span>:</p>
<p><span class="math display">\[
\begin{pmatrix}
1 &amp; 0 \\
-1 &amp; -10 \\
\end{pmatrix}
\]</span></p>
<p>And these are our gradient at <span class="math inline">\(X_(n+1)\)</span>:</p>
<p><span class="math display">\[
\begin{pmatrix}
0.4 &amp; 1.1 \\
-0.5 &amp; -0.1  \\
\end{pmatrix}
\]</span></p>
<p>The our gradients at <span class="math inline">\(X_n\)</span> will be:</p>
<p><span class="math display">\[
\begin{pmatrix}
0.4 &amp; 0 \\
 0 &amp; 0  \\
\end{pmatrix}
\]</span></p>
<div id="problems-with-vanilla-gradient" class="section level4">
<h4><span class="header-section-number">7.2.1.1</span> Problems with Vanilla Gradient</h4>
<p>Vanilla Gradient has Saturation Problem (explained in <a href="#fn78" class="footnoteRef" id="fnref78"><sup>78</sup></a>): Assuming ReLU activation layer is used, when the weighted feature sum goes below zero, the at some point the activation is capped at zero, but does not change any more, i.e. the activation is saturated. Let's say the input to the layer are two neurons with weights <span class="math inline">\(-1\)</span> and <span class="math inline">\(-1\)</span> and a bias of <span class="math inline">\(1\)</span>. Passing that through the ReLU, the activation will be neuron 1 + neuron 2 when the sum of both neurons is smaller than 1. If the sum of both is greater than one, the activation remains sasturated at an activation of 1. Also the gradient at that point will be zero, i.e. all gradient based methods will show that this neuron is not important.</p>
<p>And now my friends, you get to understand another method, more or less for free: DeconvNet</p>
</div>
</div>
<div id="deconvnet" class="section level3">
<h3><span class="header-section-number">7.2.2</span> DeconvNet</h3>
<p>DeconvNet by <a href="#fn79" class="footnoteRef" id="fnref79"><sup>79</sup></a> is almost identical to Vanilla Gradient. A DeconvNet kind of uses the same components as a Convolutional Neural Network, but reverses the operations such as filtering, pooling and activation. Apart from the ReLU layer, DeconvNet is equivalent to the Vanilla Gradient approach. Vanilla Gradient can be seen as a generalization of DeconvNet. For ReLU (<span class="math inline">\(X_{n+1}=max(X_n,0)\)</span>) the vanilla gradient computes: <span class="math inline">\(\delta{}f/\delta{}X_n=\delta{}f/\delta{}X_{n+1}\mathbb{1}(X_n&gt;0)\)</span>, where <span class="math inline">\(\mathbb{1}\)</span> is an (element-wise) indicator function. For DeconvNet ReLU, this is instead: <span class="math inline">\(R_n=R_{n+1}\mathbb{1}(R_{n+1}&gt;0)\)</span>, where <span class="math inline">\(R_n\)</span> and <span class="math inline">\(R_{n+1}\)</span> are the layer reconstructions. When doing the backpass from layer x to layer x-1, vanilla gradient &quot;remembers&quot; which of the layer x were set to zero in the forward pass and sets those to 0 in layer x-1. In DeConvNet, activation with negative value in layer x are set to zero in layer x-1. Here layers are just the activations before and after ReLU. Guided Backpropagation sets both to 0 in layer x-1: neurons that in the forward pass were 0 in layer x and neurons that are negative in the backward passe in layer x. Both DeConvNet and Guided Backpropagation do not compute the true gradient but an imputed one. If we continue our example from above, then the gradient <span class="math inline">\(X_n\)</span> (they call it layer reconstruction) becomes:</p>
<p><span class="math display">\[
\begin{pmatrix}
0.4 &amp; 1.1 \\
0 &amp; 0  \\
\end{pmatrix}
\]</span></p>
</div>
<div id="grad-cam" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Grad-CAM</h3>
<p>Grad-CAM provides visual explanations for CNN decisions. In contrast to other methods, the gradient is not backpropagated all the way back to the image, but (usually) to the last convolutional layer to produce a rough localization map that highlights important regions of the image.</p>
<p>Grad-CAM stands for Gradient-weighted Class Activation Map. And, as the name suggest it is based on the gradient of the neural networks. Grad-CAM, like other techniques, assigns some importance to each neuron, for the decision of interest. This is the class prediction (which we find in the output layer), but can, in theory, be any other layer in the neural network. Grad-CAM backpropagates this information to the last convolutional layer. Grad-CAM can be used with different CNNs: with fully-connected layers, for structured output such as captioning and in multi-task outputs and for reinforcement learning. The great thing about Grad-CAM is that besides image classification it works also for other image related tasks. This includes visual question answering and reinforcement learning.</p>
<!-- An intuitive explanation -->
<p>Let us start with an intuitive look at Grad-CAM. The goal of Grad-CAM is to understand at which parts of an image a convolutional layer &quot;looks&quot; for a certain classification. As a reminder, the first convolutional layer of a CNN takes as input the images and outputs feature maps, which encode learned features (see <a href="cnn-features.html#cnn-features">chapter Learne Features</a>). The higher-up convolutional layers do the same, but take as input the feature maps of the convolutional layer before. To understand how the CNN makes decisions, Grad-CAM analyzes where in the feature maps of the last convolutional layers relevant stuff happens. There are k feature maps in the last conv-layer, and I will call them <span class="math inline">\(A_1, A_2, \ldots, A_k\)</span>. How can we &quot;see&quot; from the feature maps, how the CNN made a certain classification? First approach, we could simply visualize the raw values of each feature map, average this over the feature maps and overlay it on our images. This would not be helpful, since the feature maps encode information for <strong>all classes</strong>, but we are interested in a certain class. This means Grad-CAM has to decide how important each of the k feature map was for our class c that we are interested in. This means we weight each pixel of each feature map by the gradient. Then we average the feature maps pixel-wise weighted by the gradient. This give us a heatmap which highlights regions that positively or negatively affect the class of interest. This heatmap is send through the ReLU function, which sets all negative values to zero. We remove all negative values, because we are only interested in the parts that contribute towards the chosen class c and not to the ones to other classes. Since we are only interest in our chosen class, they suggest to look only at the positive gradients. The word pixel here might be misleading, as the feature map is smaller than the image (because of the pooling units), but can later be mapped back to the region on the image. The we scale the Grad-CAM map to be between 0 and 1 for visualization purposes and overlay it over the original image.</p>
<!-- Pseudo code-->
<p>Here again, in Pseudo-Code and with some math. Our goal is to find the localization map defined as: <span class="math display">\[L^c_{Grad-CAM} \in \mathbb{R}^{uxv} = \underbrace{ReLU}_{\text{Pick positive values}}\left(\sum_{k} \alpha_k^c A^k\right)\]</span></p>
<p>where u is the width, v the height and c the class of interest.</p>
<ol style="list-style-type: decimal">
<li>Forward propagate the input image through the CNN</li>
<li>Obtain raw score for the class of interest, meaning the activation of the neuron before the softmax layer</li>
<li>Set all other class activations to zero</li>
<li>Backpropagate the gradient of the class of interest to the last convolutional layer before the fully connected layers <span class="math inline">\(\frac{\delta{}y^c}{\delta{}A^k}\)</span></li>
<li>Weight each feature map pixel by the gradient for the class. Index i and j refere to the width and height dimensions:</li>
</ol>
<p><span class="math display">\[\alpha_k^c = \overbrace{\frac{1}{Z}\sum_{i}\sum_{j}}^{\text{global average pooling}} \underline{\frac{\delta y^c}{\delta A_{ij}^k}}_{\text{gradients via backprop}}\]</span></p>
<p>This means that the gradients are global-average-pooled. 1. Compute an average of the feature maps, weighted per pixel by the gradient 1. Apply ReLU on averaged feature map 1. For visualization: Scale values to interval between 0 and 1. Upscale image size and overlay on original image. 1. Additional step for Guided Gra-CAM: Multiply heatmap with guided backpropagation</p>
<p>The Grad-CAM algorithm can be found here: <a href="https://keras.io/examples/vision/grad_cam/" class="uri">https://keras.io/examples/vision/grad_cam/</a> <!-- Algorithm in Detail--></p>
<p>Problems with Grad-CAM:</p>
<p>Problem of zeroing out negative gradients during backpropagation.</p>
</div>
<div id="guided-gradcam" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Guided GradCAM</h3>
<p>From the description of Grad-CAM you can see that the localization is very coarse, since the last convolutional feature maps have a much coarser resolution compared to the input image. Other attribution techniques back-propagate the entire way back to the image and each of the pixels. They are therefore much more detailed and can really show you individual edges or spots that contributed most to a prediction. A fusion of both methods is called Guided Grad-CAM. And it is super simple. You compute for an image both the Grad-CAM output and the output from, for example, Deconvolution or some other pixel-wise attribution. The Grad-CAM output is then upsampled with bilinear interpoliation, then both maps are multiplied element-wise. Grad-CAM works like a lense that focuses on certain parts of the pixel-wise attribution map.</p>
<p>TODO: Add image</p>
<p>Problem of zeroing out negative gradients during backpropagation.</p>
</div>
<div id="smoothgrad" class="section level3">
<h3><span class="header-section-number">7.2.5</span> SmoothGrad</h3>
<p>The idea of SmoothGrad by Smilkov et. al 2017 <a href="#fn80" class="footnoteRef" id="fnref80"><sup>80</sup></a> is to make gradient-based explanations less noisy by adding noise and averaging over multiple, artificially noisy, gradients. SmoothGrad is not an explanation method in itself, but can be seen as an extension to any gradient-based explanation method.</p>
<p>SmoothGrad works in the following way:</p>
<ul>
<li>Generate multiple version of the image of interest by adding noise to it</li>
<li>Create saliency maps for all images</li>
<li>Average the saliency maps</li>
</ul>
<p>Yes, it is that simple. Why should this work? The theory is that the derivative fluctuates greatly at small scales. Neural networks do not have an incentive during training to keep those derivatives smooth, their goal is to classify the images correctly. By averaging over multiple maps, these fluctuations are &quot;smoothed&quot; out.</p>
<p><span class="math display">\[E_{sg}(x)=\frac{1}{N}\sum_{i=1}^n{}E(x+g_i)\]</span></p>
<p>where <span class="math inline">\(g_i\sim{}N(0,\sigma^2)\)</span> are noise vectors sampled from the Gaussian distribution. However, the &quot;ideal&quot; noise level depends on the input image and network. The authors suggest to set it to a 10%-20% noise level, meaning that <span class="math inline">\(\frac{\sigma}{x_{max} - x_{min}}\)</span> should be between 0.1 and 0.2. The boundaries <span class="math inline">\(x_{max}\)</span> and <span class="math inline">\(x_{min}\)</span> refer to min and maximum pixel values of the image. The other parameter is the number of samples n, for which was suggested that above 50 there are diminishing returns, so 50 should be enough.</p>
</div>
<div id="alternative-perturbation-based-methods" class="section level3">
<h3><span class="header-section-number">7.2.6</span> Alternative: Perturbation-based Methods</h3>
<p>An alternative to gradient-based methods are perturbation-based method. Perturbation-based methods treat the neural network as a black box. They work by manipulating the input images and observing how the output changes. Two of those methods are covered in this book: Shapley in the <a href="Shap%20Chapter">#shap</a> and LIME in the <a href="LIME%20Chapter">#lime</a>. They are more expensive to compute usually, because multiple predictions are needed. But they have the advantage that they are model-agnostic.</p>
</div>
<div id="examples-7" class="section level3">
<h3><span class="header-section-number">7.2.7</span> Examples</h3>
<p>Let's see some example how these maps look like and how the methods compare, qualitatively. The network in question is VGG-16 (Simonyan et. al 2014 <a href="#fn81" class="footnoteRef" id="fnref81"><sup>81</sup></a>) which was trained on the ImageNet data, and therefore able to distinguish more than 20,000 categories. We start with the following images. For all of these images, we will have a look at the top classification and create explanations for this top class.</p>
<p>These are the images:</p>
<p><img src="images/original-images-classification.png" width="80%" /></p>
<p>The left image with the honorable dog guarding the Interpretable Machine Learning book got a classification &quot;Greyhound&quot; with a probability score of 35%. The image in the middle shows a bowl of yummy ramen soup and is correctly classified as &quot;Soup Bowl&quot; with probability of 50% The third image shows an octopus on the ocean floor, with an incorrect classification as &quot;Eel&quot; with high confidence probability of 70%</p>
<p><img src="images/smoothgrad.png" width="80%" /></p>
<p>Unfortunately, it is a bit of a mess. But let us have a look at the individual explanations, starting with the dog. The vanilla method and the version with smoothgrad applied both highlight the dog the most, which makes sense. But also some areas around the book are highlighted, which is odd. Grad-Cam highlights only the book area, which does not make any sense at all. And from here on, it gets a bit messier. The vanilla method seems to fail for both the soup bowl and the octopus (or, as the network thinks, eel). Both images look like the after impression of looking into the sun for too long. (Please do not look into the sun). Smoothgrad helps a lot, at least the areas are more defined. In the soup example some of the ingredeints are highlighted, such as the eggs and the meat, but also the area around the chop sticks. For the octopus image, mostly the animal itself is highlighted. Grad-Cam is all over the place. For the soup bowl, the egg part and, for some reason, the upper part of the bowl are highlighted. The octopus images are even messier.</p>
<p>You can already see the difficulties here in assessing whether we trust the explanations. As a first step, we have to think about which parts of the image contains information that are relevant to the images content and the classification. But then we also have to think about what the neural network might have used for the classification. Maybe the soup bowl was classified correctly because of the combination of eggs and chopstick, as smoothgrad implies? Or it detected the shape of the bow plus some ingredients as maybe Grad-Cam implies? We just do not know.</p>
<p>And that is the big issue with all of these methods. We do not have a groundtruth explanation. We can, as a first step, only reject explanation that make clearly no sense (and even in this step we have no strong confidence, I mean, how the prediction process is very complicated in the neural network).</p>
<p>There are approachs to investigate how robust and correct the explanations are. They give us some hints that the explanations sometims are even independent of network and data, which is really bad.</p>
</div>
<div id="advantages-18" class="section level3">
<h3><span class="header-section-number">7.2.8</span> Advantages</h3>
<ul>
<li>The explanations are visual and easy to understand. Especially when methods only highlight important pixels, it becomes easy to understand that these were the most important regions fo the image.</li>
<li>Methods that rely on gradient are usually faster to compute than model-agnostic methods. For example, <a href="lime.html#lime">LIME</a> and <a href="shap.html#shap">SHAP</a> can also be used, but are more expensive to compute.</li>
<li>There are many methods to choose from.</li>
</ul>
</div>
<div id="disadvantages-18" class="section level3">
<h3><span class="header-section-number">7.2.9</span> Disadvantages</h3>
<ul>
<li>As with most interpretation methods, it is difficult to know whether the explanation is <strong>correct</strong>, and a huge part of the evaluation is just qualitative (&quot;These explanations look about right, let's publish the paper already.&quot;).</li>
<li>Saliency maps can be very fragile. <a href="#fn82" class="footnoteRef" id="fnref82"><sup>82</sup></a> showed that introducing small (adversarial) perturbations to an image, that still lead to the same prediction can lead to very different pixels that are highlighted as explanation.</li>
<li>Also <a href="#fn83" class="footnoteRef" id="fnref83"><sup>83</sup></a> showed that these methods can be highly unreliable. They added a constant shift to the input data, i.e. they added the same pixel changes to all images. They compare two networks, original network and the one were the bias of the first layer is changes to adapt for the constant pixel shift, so that the activations of both networks are the same and so are the predictions. Also the gradient is the same for both. This means network 2 simply cancels out the constant shift in the first layer. They looked at DeepLift, Vanilla (Simple) Gradient and Integrated Gradients.</li>
<li>The paper &quot;Sanity Checks for Saliency Maps&quot; <a href="#fn84" class="footnoteRef" id="fnref84"><sup>84</sup></a> investigated whether saliency methods are insensitive to model and data. Insensitivity is highly undesirable, because this means the &quot;explanation&quot; is not related to the the model or the data. They compared methods that failed the checks to edge detectors which simply highlight strong pixel color changes in images and are not related to any prediction model or abstract features of the image, and requires not training. They checked the methods Vanilla Gradient, Gradient x Input, Integrated Gradients, Guided Backpropagation, Guided GradCAM and SmoothGrad (with vanilla gradient). Vanilla gradients and GradCAM pass the check, while Guided Backpropagation and Guided GradCAM fail.</li>
<li>However, the sanity checks paper itself has found some critiques with a paper called &quot;Sanity Checks for Saliency Metrics&quot; <a href="#fn85" class="footnoteRef" id="fnref85"><sup>85</sup></a> (of course). They found that there is a lack of consistency for evaluation metrics (I know, it is getting quite meta now). So we are back to where we started ... It remains difficult to evaluate this visual explanations. This makes it very difficult for a practitioner of course, because there is no obvious choice of method.</li>
<li>All in all, it is a very unsatisfying state of affairs. We need to wait a bit longer for more research on this matter. And please, no more invention of new methods, but rather more scrutiny of how to evaluate these methods.</li>
</ul>
</div>
<div id="software-5" class="section level3">
<h3><span class="header-section-number">7.2.10</span> Software</h3>
<p>There are various implementations of feature attribution methods. For the example, I used <a href="https://pypi.org/project/tf-keras-vis/">tf-keras-vis</a>. One of the most comprehensive libraries is <a href="https://github.com/albermax/innvestigate">iNNvestigate</a>, which implements vanilla gradient, smoothgrad, Deconvnet, Guided Backpropagation, PatternNet, LRP, ... A lot of the methods are implemented in the DeepExplain Toolbox: <a href="https://github.com/marcoancona/DeepExplain" class="uri">https://github.com/marcoancona/DeepExplain</a></p>
<p>[^lrp] Bach, Sebastian, et al. &quot;On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.&quot; PloS one 10.7 (2015).</p>
<!-- References about problems -->
<!-- Toolboxes -->

</div>
</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="76">
<li id="fn76"><p>Kindermans, Pieter-Jan, et al. &quot;The (un) reliability of saliency methods.&quot; Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer, Cham, 2019. 267-280. <a href="https://arxiv.org/pdf/1711.00867v1.pdf" class="uri">https://arxiv.org/pdf/1711.00867v1.pdf</a><a href="feature-attribution-for-image-classification.html#fnref76">↩</a></p></li>
<li id="fn77"><p>Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. &quot;Deep inside convolutional networks: Visualising image classification models and saliency maps.&quot; arXiv preprint arXiv:1312.6034 (2013).<a href="feature-attribution-for-image-classification.html#fnref77">↩</a></p></li>
<li id="fn78"><p>Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. &quot;Learning important features through propagating activation differences.&quot; Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.<a href="feature-attribution-for-image-classification.html#fnref78">↩</a></p></li>
<li id="fn79"><p>Zeiler, Matthew D., and Rob Fergus. &quot;Visualizing and understanding convolutional networks.&quot; European conference on computer vision. Springer, Cham, 2014.<a href="feature-attribution-for-image-classification.html#fnref79">↩</a></p></li>
<li id="fn80"><p>Smilkov, Daniel, et al. &quot;Smoothgrad: removing noise by adding noise.&quot; arXiv preprint arXiv:1706.03825 (2017).<a href="feature-attribution-for-image-classification.html#fnref80">↩</a></p></li>
<li id="fn81"><p>Simonyan, Karen, and Andrew Zisserman. &quot;Very deep convolutional networks for large-scale image recognition.&quot; arXiv preprint arXiv:1409.1556 (2014).<a href="feature-attribution-for-image-classification.html#fnref81">↩</a></p></li>
<li id="fn82"><p>Ghorbani, Amirata, Abubakar Abid, and James Zou. &quot;Interpretation of neural networks is fragile.&quot; Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.<a href="feature-attribution-for-image-classification.html#fnref82">↩</a></p></li>
<li id="fn83"><p>Kindermans, Pieter-Jan, et al. &quot;The (un) reliability of saliency methods.&quot; Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer, Cham, 2019. 267-280. <a href="https://arxiv.org/pdf/1711.00867v1.pdf" class="uri">https://arxiv.org/pdf/1711.00867v1.pdf</a><a href="feature-attribution-for-image-classification.html#fnref83">↩</a></p></li>
<li id="fn84"><p>Adebayo, Julius, et al. &quot;Sanity checks for saliency maps.&quot; Advances in Neural Information Processing Systems. 2018.<a href="feature-attribution-for-image-classification.html#fnref84">↩</a></p></li>
<li id="fn85"><p>Harborne, Daniel, et al. &quot;Sanity Checks for Saliency Metrics.&quot; arXiv preprint arXiv:192.01451<a href="feature-attribution-for-image-classification.html#fnref85">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cnn-features.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="future.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/07.3-feature-attribution.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
